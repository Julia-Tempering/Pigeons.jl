<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Distributed usage (MPI) · Pigeons.jl</title><meta name="title" content="Distributed usage (MPI) · Pigeons.jl"/><meta property="og:title" content="Distributed usage (MPI) · Pigeons.jl"/><meta property="twitter:title" content="Distributed usage (MPI) · Pigeons.jl"/><meta name="description" content="Documentation for Pigeons.jl."/><meta property="og:description" content="Documentation for Pigeons.jl."/><meta property="twitter:description" content="Documentation for Pigeons.jl."/><meta property="og:url" content="https://Julia-Tempering.github.io/Pigeons.jl/mpi/"/><meta property="twitter:url" content="https://Julia-Tempering.github.io/Pigeons.jl/mpi/"/><link rel="canonical" href="https://Julia-Tempering.github.io/Pigeons.jl/mpi/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Pigeons.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Basic usage (local)</a></li><li><a class="tocitem" href="../unidentifiable-example/">Why parallel tempering (PT)?</a></li><li><a class="tocitem" href="../parallel/">Parallelization</a></li><li class="is-active"><a class="tocitem" href>Distributed usage (MPI)</a><ul class="internal"><li><a class="tocitem" href="#mpi-local"><span>Running MPI locally</span></a></li><li><a class="tocitem" href="#Running-MPI-on-a-cluster"><span>Running MPI on a cluster</span></a></li><li><a class="tocitem" href="#Code-dependencies"><span>Code dependencies</span></a></li><li><a class="tocitem" href="#Details-on-setting-up-Pigeons-with-multi-node-MPI-#details-on-setting-up-mpi"><span>Details on setting up Pigeons with multi-node MPI #details-on-setting-up-mpi</span></a></li></ul></li><li><a class="tocitem" href="../variational/">Variational PT</a></li><li><span class="tocitem">Supported inputs</span><ul><li><a class="tocitem" href="../input-overview/">Inputs overview</a></li><li><a class="tocitem" href="../input-turing/">Turing.jl model</a></li><li><a class="tocitem" href="../input-julia/">Black-box function</a></li><li><a class="tocitem" href="../input-stan/">Stan model</a></li><li><a class="tocitem" href="../input-nonjulian/">Non-julian MCMC</a></li><li><a class="tocitem" href="../input-explorers/">Custom MCMC</a></li></ul></li><li><span class="tocitem">Outputs</span><ul><li><a class="tocitem" href="../output-overview/">Outputs overview</a></li><li><a class="tocitem" href="../output-inferencereport/">Automated reports</a></li><li><a class="tocitem" href="../output-reports/">Standard output</a></li><li><a class="tocitem" href="../output-traces/">Traces</a></li><li><a class="tocitem" href="../output-plotting/">Plots</a></li><li><a class="tocitem" href="../output-normalization/">log(Z)</a></li><li><a class="tocitem" href="../output-numerical/">Numerical</a></li><li><a class="tocitem" href="../output-online/">Online stats</a></li><li><a class="tocitem" href="../output-off-memory/">Off-memory</a></li><li><a class="tocitem" href="../output-pt/">PT diagnostics</a></li><li><a class="tocitem" href="../output-custom-types/">Custom types</a></li><li><a class="tocitem" href="../output-extended/">Extended output</a></li><li><a class="tocitem" href="../output-mpi-postprocessing/">MPI output</a></li></ul></li><li><a class="tocitem" href="../checkpoints/">Checkpoints</a></li><li><a class="tocitem" href="../correctness/">Correctness checks</a></li><li><a class="tocitem" href="../pt/">More on PT</a></li><li><a class="tocitem" href="../distributed/">More on distributed PT</a></li><li><a class="tocitem" href="../interfaces/">Interfaces</a></li><li><a class="tocitem" href="../reference/">Reference</a></li><li><a class="tocitem" href="../developers/">For developers</a></li><li><a class="tocitem" href="../openings/">Openings</a></li><li><a class="tocitem" href="../about-us/">About Us</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Distributed usage (MPI)</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Distributed usage (MPI)</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Julia-Tempering/Pigeons.jl/blob/main/docs/src/mpi.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Distributed-sampling-over-MPI-using-Pigeons"><a class="docs-heading-anchor" href="#Distributed-sampling-over-MPI-using-Pigeons">Distributed sampling over MPI using Pigeons</a><a id="Distributed-sampling-over-MPI-using-Pigeons-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-sampling-over-MPI-using-Pigeons" title="Permalink"></a></h1><h2 id="mpi-local"><a class="docs-heading-anchor" href="#mpi-local">Running MPI locally</a><a id="mpi-local-1"></a><a class="docs-heading-anchor-permalink" href="#mpi-local" title="Permalink"></a></h2><p>To run MPI locally on one machine, using 4 MPI processes, use:</p><pre><code class="language-julia hljs">using Pigeons
result = pigeons(
    target = toy_mvn_target(100),
    checkpoint = true,
    on = ChildProcess(
            n_local_mpi_processes = 4))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Result{PT}(&quot;/home/runner/work/Pigeons.jl/Pigeons.jl/docs/build/results/all/2025-04-17-18-09-46-lArTkJRq&quot;)</code></pre><p>Note that if <code>n_local_mpi_processes</code> exceeds the number of cores, performance  will steeply degrade (in contrast to threads, for which performance degrades  much more gracefully when the number of threads exceeds the number of cores). </p><p>Using <code>on = ChildProcess(...)</code> is also useful to change the  number of threads without having to restart the Julia session.  For example, to start 4 child processes, each with two threads concurrently sharing work  across the chains, use:</p><pre><code class="language-julia hljs">result = pigeons(
    target = toy_mvn_target(100),
    multithreaded = true,
    checkpoint = true,
    on = ChildProcess(
            n_local_mpi_processes = 4,
            n_threads = 2))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Result{PT}(&quot;/home/runner/work/Pigeons.jl/Pigeons.jl/docs/build/results/all/2025-04-17-18-10-15-sZHFP4e5&quot;)</code></pre><p>Alternatively, if instead of using the 2 threads to parallelize across chain, we want to use them to parallelize e.g. a custom likelihood evalutation over datapoints, set <code>multithreaded = false</code> to  indicate to pigeons it is not responsible for the multithreading (<code>multithreaded = false</code> is the default behaviour):</p><pre><code class="language-julia hljs">result = pigeons(
    target = toy_mvn_target(100),
    multithreaded = false, # can be skipped, the default
    checkpoint = true,
    on = ChildProcess(
            n_local_mpi_processes = 4,
            n_threads = 2))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Result{PT}(&quot;/home/runner/work/Pigeons.jl/Pigeons.jl/docs/build/results/all/2025-04-17-18-10-44-UjUCUigy&quot;)</code></pre><p>To analyze the output, see the documentation page on <a href="../output-mpi-postprocessing/#output-mpi-postprocessing">post-processing for MPI runs</a>. Briefly, one option is to load the state of the sampler  back to your interactive chain via: </p><pre><code class="language-julia hljs">pt = Pigeons.load(result) # possible thanks to &#39;pigeons(..., checkpoint = true)&#39; used above</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">PT(&quot;/home/runner/work/Pigeons.jl/Pigeons.jl/docs/build/results/all/2025-04-17-18-11-13-n3UtWwfi&quot;)</code></pre><h2 id="Running-MPI-on-a-cluster"><a class="docs-heading-anchor" href="#Running-MPI-on-a-cluster">Running MPI on a cluster</a><a id="Running-MPI-on-a-cluster-1"></a><a class="docs-heading-anchor-permalink" href="#Running-MPI-on-a-cluster" title="Permalink"></a></h2><div class="admonition is-info"><header class="admonition-header">The magic of distributed Parallel Tempering</header><div class="admonition-body"><p>If the dimensionality of the state space is large, you may worry that  the time to transmit states over the network would dominate the running time.  Remarkably, the size of the messages transmitted in the inner loop of our  algorithm does <strong>not</strong> depend on the state space. In a nutshell, the  machines only need to transmit the value of log density ratios (a single float).  See <a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12464">Algorithm 5 in Syed et al., 2021</a> for details.</p></div></div><p>MPI is typically available via a cluster scheduling system. At the time of  writing, <a href="https://github.com/openpbs/openpbs">PBS</a> and  <a href="https://slurm.schedmd.com/documentation.html">SLURM</a> are supported,  and an experimental implementation of <a href="https://www.ibm.com/docs/en/spectrum-lsf/10.1.0?topic=overview-lsf-introduction">LSF</a> is included.  Create an issue if you would like another submission system included. </p><p>The main three steps to run MPI over several machines are given below.  For more information, please read <a href="#details-on-setting-up-mpi">the detailed instructions</a>.</p><ol><li>In the cluster login node, follow the <a href="../#installing-pigeons">local installation instructions</a>. </li><li>Start Julia in the login node, and perform a one-time setup. Read the documentation at <a href="../reference/#Pigeons.setup_mpi-Tuple{}"><code>setup_mpi()</code></a> for more information. </li><li>Still in the Julia REPL running in the login node, use the following syntax:</li></ol><pre><code class="nohighlight hljs">mpi_run = pigeons(
    target = toy_mvn_target(1000000), 
    n_chains = 1000,
    checkpoint = true,
    on = MPIProcesses(
        n_mpi_processes = 1000,
        n_threads = 1))</code></pre><p>This will start a distributed PT algorithm with 1000 chains on 1000 MPIProcesses processes, each using one thread, targeting a one million  dimensional target distribution. On the UBC Sockeye cluster, the last  round of this run (i.e. the last 1024 iterations) takes 10 seconds to complete, versus more than  2 hours if run serially, i.e. a &gt;700x speed-up.  This is reasonably close to the theoretical 1000x speedup, i.e. we see that the communication costs are negligible. </p><p>You can &quot;watch&quot; the progress of your job (queue status and  standard output once it is available), using:</p><pre><code class="nohighlight hljs">watch(mpi_run)</code></pre><p>and cancel/kill a job using </p><pre><code class="nohighlight hljs">kill_job(mpi_run)</code></pre><p>To analyze the output, see the documentation page on <a href="../output-mpi-postprocessing/#output-mpi-postprocessing">post-processing for MPI runs</a>. In a nutshell, one option is to load the state of the sampler  back to your interactive chain via: </p><pre><code class="nohighlight hljs">pt = Pigeons.load(mpi_run) # possible thanks to &#39;pigeons(..., checkpoint = true)&#39; used above</code></pre><h2 id="Code-dependencies"><a class="docs-heading-anchor" href="#Code-dependencies">Code dependencies</a><a id="Code-dependencies-1"></a><a class="docs-heading-anchor-permalink" href="#Code-dependencies" title="Permalink"></a></h2><p>So far we have used examples where the target, explorers, etc  are built-in inside the Pigeons module.  However in typical use cases, some user-provided code needs to be provided to  <a href="../reference/#Pigeons.ChildProcess"><code>ChildProcess</code></a>  and <a href="../reference/#Pigeons.MPIProcesses"><code>MPIProcesses</code></a> so that the other participating Julia  processes have access to it.  This is done with the argument <code>dependencies</code> (of type <code>Vector</code>;  present in  both <a href="../reference/#Pigeons.ChildProcess"><code>ChildProcess</code></a>  and <a href="../reference/#Pigeons.MPIProcesses"><code>MPIProcesses</code></a>).  Two types of elements can be used in the vector of dependencies, and they can be mixed:</p><ul><li>elements of type <code>Module</code>: for each of those, an <code>using</code> statement will be generated in the script used by the child process;</li><li>elements of type <code>String</code>: a path to a Julia file defining functions and types, for each of those an <code>include</code> call is generated. </li></ul><p>Here is an example where we run a custom Ising model in a child process:</p><pre><code class="language-julia hljs">using Pigeons

# making the path absolute can be necessary in some contexts:
ising_path = pkgdir(Pigeons) * &quot;/examples/ising.jl&quot;
lazy_path = pkgdir(Pigeons) * &quot;/examples/lazy-ising.jl&quot;

pigeons(
    # see examples/lazy-ising.jl why we need Lazy (Documenter.jl-specific issue)
    target = Pigeons.LazyTarget(Val(:IsingLogPotential)),
    checkpoint = true,
    on = ChildProcess(
            n_local_mpi_processes = 2,
            dependencies = [
                Pigeons, # &lt;- Pigeons itself can be skipped, added automatically
                ising_path, # &lt;- these are needed for this example to work
                lazy_path   # &lt;--+
            ]

        )
    )</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Result{PT}(&quot;/home/runner/work/Pigeons.jl/Pigeons.jl/docs/build/results/all/2025-04-17-18-11-13-CcC8Fnea&quot;)</code></pre><p>Note the use of <code>LazyTarget(..)</code>.  When starting a child process, the arguments of <code>pigeons(...)</code> are used to create  an <a href="../reference/#Pigeons.Inputs"><code>Inputs</code></a> struct, which is serialized.  In certain corner cases this serialization may not be possible, for example if the  target depends on external processes, or here due to the fact that Documenter.jl  defines temporary environments (see examples/lazy-ising.jl for details). In these corner cases, you can use a <a href="../reference/#Pigeons.LazyTarget"><code>LazyTarget</code></a> to delay the creation of the  target so that it is performed in the child processes instead of the calling process.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>In order for the child processes to be able to load the same module versions as  the current process, the current process calls <code>Base.active_project()</code> and  pass that information to the child processes. The child processes will activate  that environment before proceeding to sampling.</p><p>We therefore assume that the environment given by <code>Base.active_project()</code> is  in working order.</p></div></div><h2 id="Details-on-setting-up-Pigeons-with-multi-node-MPI-#details-on-setting-up-mpi"><a class="docs-heading-anchor" href="#Details-on-setting-up-Pigeons-with-multi-node-MPI-#details-on-setting-up-mpi">Details on setting up Pigeons with multi-node MPI #details-on-setting-up-mpi</a><a id="Details-on-setting-up-Pigeons-with-multi-node-MPI-#details-on-setting-up-mpi-1"></a><a class="docs-heading-anchor-permalink" href="#Details-on-setting-up-Pigeons-with-multi-node-MPI-#details-on-setting-up-mpi" title="Permalink"></a></h2><p>We provide more details here to get Pigeons to work on HPC clusters with MPI,  specifically to  allow Pigeons processes across several machines to communicate with each  other. </p><h3 id="Understanding-your-HPC-cluster"><a class="docs-heading-anchor" href="#Understanding-your-HPC-cluster">Understanding your HPC cluster</a><a id="Understanding-your-HPC-cluster-1"></a><a class="docs-heading-anchor-permalink" href="#Understanding-your-HPC-cluster" title="Permalink"></a></h3><p>Read the documentation of your HPC cluster or contact the administrator to  find answers to the following questions:</p><ul><li>What are the locations in the file system that are shared between nodes?    Which ones are read/write vs read only? </li><li>Login nodes and compute nodes will often behave differently.    In particular they might have different read/write access in the various volumes. </li><li>Are there HPC modules that need to be loaded to run MPI jobs?</li><li>Optional: Is there a Julia install provided (e.g., vi HPC modules)? </li><li>Optional: is there an example showing how to use MPI.jl? </li></ul><h3 id="Installing-Julia-on-HPC"><a class="docs-heading-anchor" href="#Installing-Julia-on-HPC">Installing Julia on HPC</a><a id="Installing-Julia-on-HPC-1"></a><a class="docs-heading-anchor-permalink" href="#Installing-Julia-on-HPC" title="Permalink"></a></h3><p>Check first if an HPC module is available with a recent version of Julia.  If not, it is easy to install  one yourself (no root access needed). We explain how to in this section.</p><p>As of 2024, we have encountered issues with <code>juliaup</code>   on HPC and recommend instead a simple approach:</p><ul><li>Create a <code>bin</code> directory in a volume that is readable on all nodes.    E.g., it could be <code>~/bin</code>. Go to that directory with <code>cd</code>. </li><li>Follow <a href="https://julialang.org/downloads/platform/#linux_and_freebsd">these instructions</a>,    including the step on how to add Julia to your <code>PATH</code> variable in <code>~/.bashrc</code>. </li></ul><h3 id="The-Julia-depot"><a class="docs-heading-anchor" href="#The-Julia-depot">The Julia depot</a><a id="The-Julia-depot-1"></a><a class="docs-heading-anchor-permalink" href="#The-Julia-depot" title="Permalink"></a></h3><p>Julia&#39;s package manager (Pkg.jl) stores a large number of files in a  directory called the <em>Julia depot</em>. Julia will look for the envirnonment variable  <code>JULIA_DEPOT</code> to find that directory. </p><p>The standard approach is to have one such Julia depot per user in a shared (network) drive with read and write access from all nodes.  However, having many files in a shared drive can make the Pkg operations and  pre-compilation extremely slow. If you see this issue, two possible options:</p><ul><li>If your HPC architecture has a <em>burst buffer</em>, this will be a good place to    locate the Julia depot. You may need to request allocation, but it is well worth    doing so as it creates a huge performance boost on Pkg and precompile operations.</li><li>If not, a workaround is described <a href="https://github.com/UBC-Stat-ML/zip_depot">in this page</a>.</li></ul><h3 id="Setting-up-a-Julia-project"><a class="docs-heading-anchor" href="#Setting-up-a-Julia-project">Setting up a Julia project</a><a id="Setting-up-a-Julia-project-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-up-a-Julia-project" title="Permalink"></a></h3><p>It should be in a volume with read/write access from all nodes. For testing purpose, it can simply be an empty directory.  <code>cd</code> into it and start julia.</p><p>Activate the project and install Pigeons in it by using:</p><pre><code class="nohighlight hljs">] activate . 
add Pigeons</code></pre><h3 id="Load-MPI-modules"><a class="docs-heading-anchor" href="#Load-MPI-modules">Load MPI modules</a><a id="Load-MPI-modules-1"></a><a class="docs-heading-anchor-permalink" href="#Load-MPI-modules" title="Permalink"></a></h3><p>During the MPI setup process (next step), the MPI library will need  to be loaded in order for Pigeons to find it (more precisely, Pigeons will  call MPIPreferences.jl). </p><p>To see if you need to do this, try <code>which mpiexec</code>: if it finds mpiexec,  you are probably good to go and can go to next step, otherwise, read  the cluster documentation or talk to the cluster administrator.</p><p>This is system-dependent, so it might be done by default, or may  require loading certain modules, e.g., on certain systems this may look like:</p><pre><code class="nohighlight hljs">module load gcc
module load openmpi</code></pre><p>Keep note of the list of modules needed, you will need it later in the process.</p><h3 id="Setting-up-Pigeons-MPI"><a class="docs-heading-anchor" href="#Setting-up-Pigeons-MPI">Setting up Pigeons MPI</a><a id="Setting-up-Pigeons-MPI-1"></a><a class="docs-heading-anchor-permalink" href="#Setting-up-Pigeons-MPI" title="Permalink"></a></h3><p>We now need to tell Pigeons how to bind to the HPC&#39;s MPI.  This needs to be done only once per project. </p><h4 id="Presets"><a class="docs-heading-anchor" href="#Presets">Presets</a><a id="Presets-1"></a><a class="docs-heading-anchor-permalink" href="#Presets" title="Permalink"></a></h4><p>Look first at the list of clusters that have &quot;presets&quot; available, by  typing <code>Pigeons.setup_mpi_</code> and then tab. These presets are the most straightforward to use. If there is a preset available for your system, just run that command and you  are done! </p><p>For example, on most Digital Research Alliance of Canada HPC clusters (formerly Compute Canada), you can simply use:</p><pre><code class="nohighlight hljs">Pigeons.setup_mpi_compute_canada()</code></pre><h4 id="Calling-[setup_mpi()](@ref)"><a class="docs-heading-anchor" href="#Calling-[setup_mpi()](@ref)">Calling <a href="../reference/#Pigeons.setup_mpi-Tuple{}"><code>setup_mpi()</code></a></a><a id="Calling-[setup_mpi()](@ref)-1"></a><a class="docs-heading-anchor-permalink" href="#Calling-[setup_mpi()](@ref)" title="Permalink"></a></h4><p>If a preset is not available, manual configuration can be done using  <a href="../reference/#Pigeons.setup_mpi-Tuple{}"><code>Pigeons.setup_mpi()</code></a>. To get more information on the  arguments to pass in to <code>Pigeons.setup_mpi()</code>, see <a href="../reference/#Pigeons.MPISettings"><code>MPISettings</code></a>, but we walk over the main steps here. </p><h5 id="Submission-system"><a class="docs-heading-anchor" href="#Submission-system">Submission system</a><a id="Submission-system-1"></a><a class="docs-heading-anchor-permalink" href="#Submission-system" title="Permalink"></a></h5><p>The argument <code>submission_system</code> should specify the queue  submission system. Most popular choices are <code>:pbs</code> and  <code>:slurm</code>. Pigeons will use this information to generate  the queue submission scripts.</p><p>Optionally, you can use also <code>add_to_submission</code> to add  extra information in the queue submission script. </p><p>See also <a href="https://github.com/Julia-Tempering/Pigeons.jl/blob/main/src/submission/presets.jl">presets.jl</a> for examples of  what this looks like in different existing systems.  When you submit MPI jobs, you can see the generated script  in <code>results/latest/.submission_script.sh</code>. </p><p>Here is an example of what a generated script  looks like if we add  <code>add_to_submission = [&quot;source ~/bin/zip_depot/bin/load_depot&quot;]</code>  as needed for using <a href="https://github.com/UBC-Stat-ML/zip_depot">the zip_depot utility</a>: </p><pre><code class="nohighlight hljs">#!/bin/bash
#SBATCH -t 00:05:00
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=1
#SBATCH --mem-per-cpu=8gb 

#SBATCH --job-name=2025-04-09-18-04-55-fltVoqC0
#SBATCH -o /home/bouchar3/Pigeons.jl/results/all/2025-04-09-18-04-55-fltVoqC0/info/stdout.txt
#SBATCH -e /home/bouchar3/Pigeons.jl/results/all/2025-04-09-18-04-55-fltVoqC0/info/stderr.txt
source ~/bin/zip_depot/bin/load_depot # &lt;-- this is where &#39;add_to_submission&#39; entries are added

cd $SLURM_SUBMIT_DIR
module load julia/1.11.3
MPI_OUTPUT_PATH=&quot;/home/bouchar3/Pigeons.jl/results/all/2025-04-09-18-04-55-fltVoqC0&quot;

mpiexec --output-filename &quot;$MPI_OUTPUT_PATH/mpi_out&quot; --merge-stderr-to-stdout   /cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Core/julia/1.11.3/bin/julia -C native -J/cvmfs/soft.computecanada.ca/easybuild/software/2023/x86-64-v3/Core/julia/1.11.3/lib/julia/sys.so -g1 --startup-file=no --banner=no --project=/home/bouchar3/Pigeons.jl --threads=1 --compiled-modules=existing /home/bouchar3/Pigeons.jl/results/all/2025-04-09-18-04-55-fltVoqC0/.launch_script.jl</code></pre><h5 id="Environment-modules"><a class="docs-heading-anchor" href="#Environment-modules">Environment modules</a><a id="Environment-modules-1"></a><a class="docs-heading-anchor-permalink" href="#Environment-modules" title="Permalink"></a></h5><p>The HPC modules you are currently using for the setup will need  to be added to the generated script, so Pigeons needs to know about them. Add them to the <code>environment_modules</code> argument of  <code>setup_mpi()</code>. </p><h5 id="Library-name"><a class="docs-heading-anchor" href="#Library-name">Library name</a><a id="Library-name-1"></a><a class="docs-heading-anchor-permalink" href="#Library-name" title="Permalink"></a></h5><p>In most cases, the MPI system library is found automatically, so try  first leaving <code>library_name</code> to its default value of <code>nothing</code>.  If not, see the documentation in <a href="../reference/#Pigeons.MPISettings"><code>MPISettings</code></a> under  <code>library_name</code>. </p><h5 id="Customizing-the-mpiexec-command"><a class="docs-heading-anchor" href="#Customizing-the-mpiexec-command">Customizing the mpiexec command</a><a id="Customizing-the-mpiexec-command-1"></a><a class="docs-heading-anchor-permalink" href="#Customizing-the-mpiexec-command" title="Permalink"></a></h5><p>In many HPC clusters, the command <code>mpiexec</code> is used to submit jobs to MPI.  This is the default value in Pigeons&#39; generated submission scripts.  In other clusters, a different command is used.  We describe here how to perform this customization. </p><p>The main mechanism is the argument <code>mpiexec</code> specified when calling <a href="../reference/#Pigeons.setup_mpi-Tuple{}"><code>setup_mpi()</code></a>,  for example, on some cluster you may need:</p><pre><code class="nohighlight hljs">Pigeons.setup_mpi(
    mpiexec = &quot;srun -n \$SLURM_NTASKS --mpi=pmi2&quot;,
    ...
)</code></pre><p>Minor note: in order to be able to use the convenience function  <a href="../reference/#Pigeons.watch-Tuple{Result}"><code>watch()</code></a>, used to show standard output of MPI jobs, you need  to ensure MPI will create output files at the right location.  For mpiexec, this is achieved with the default arguments of <code>mpiexec</code>:  see the source  code of <a href="../reference/#Pigeons.MPISettings"><code>MPISettings</code></a>. </p><p>If you need to change argument for a single job,  additional arguments to <code>mpiexec</code> (or alternatives such as <code>srun</code>)  can be provided in the argument <code>mpiexec_args</code> in <a href="../reference/#Pigeons.MPIProcesses"><code>MPIProcesses</code></a>. </p><h4 id="Testing-your-MPI-setup"><a class="docs-heading-anchor" href="#Testing-your-MPI-setup">Testing your MPI setup</a><a id="Testing-your-MPI-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Testing-your-MPI-setup" title="Permalink"></a></h4><p>Use the following to start MPI over two MPI processes for  quick testing:</p><pre><code class="nohighlight hljs">using Pigeons 
result = pigeons(
            target = toy_mvn_target(10), 
            on = MPIProcesses(walltime = &quot;00:05:00&quot;), 
            checkpoint = true)</code></pre><p>Then use: </p><pre><code class="nohighlight hljs">watch(result)</code></pre><p>To see the output. You can also look at the following  files to help you troubleshoot potential issues, all found  in <code>results/all/latest</code> (or <code>results/all/[time]/</code>):</p><ul><li><code>.submission_script.sh</code>: the file submitted to the queue,</li><li><code>.launch_script.jl</code>: the script started on each node,</li><li><code>info/submission_output.txt</code>: the output of submitting the job to the queue,</li><li><code>info/stderr.txt</code> and <code>info/stdout.txt</code>: the slurm/pbs output,</li><li><code>mpi_out</code>: the mpiexec output, organized by node (internally, Pigeons suppresses most output on all nodes except the one at rank 0).  </li></ul><h4 id="Creating-a-PR-with-your-cluster&#39;s-setup"><a class="docs-heading-anchor" href="#Creating-a-PR-with-your-cluster&#39;s-setup">Creating a PR with your cluster&#39;s setup</a><a id="Creating-a-PR-with-your-cluster&#39;s-setup-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-a-PR-with-your-cluster&#39;s-setup" title="Permalink"></a></h4><p>Once you have determined what options to pass in to  <a href="../reference/#Pigeons.setup_mpi-Tuple{Pigeons.MPISettings}"><code>setup_mpi</code></a>, please consider creating a Pull Request  adding one function in the file  <a href="https://github.com/Julia-Tempering/Pigeons.jl/blob/main/src/submission/presets.jl">presets.jl</a>. Thank you!</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../parallel/">« Parallelization</a><a class="docs-footer-nextpage" href="../variational/">Variational PT »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.10.1 on <span class="colophon-date" title="Thursday 17 April 2025 18:15">Thursday 17 April 2025</span>. Using Julia version 1.11.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
