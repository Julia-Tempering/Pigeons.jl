<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>More on distributed PT · Pigeons.jl</title><meta name="title" content="More on distributed PT · Pigeons.jl"/><meta property="og:title" content="More on distributed PT · Pigeons.jl"/><meta property="twitter:title" content="More on distributed PT · Pigeons.jl"/><meta name="description" content="Documentation for Pigeons.jl."/><meta property="og:description" content="Documentation for Pigeons.jl."/><meta property="twitter:description" content="Documentation for Pigeons.jl."/><meta property="og:url" content="https://Julia-Tempering.github.io/Pigeons.jl/distributed/"/><meta property="twitter:url" content="https://Julia-Tempering.github.io/Pigeons.jl/distributed/"/><link rel="canonical" href="https://Julia-Tempering.github.io/Pigeons.jl/distributed/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">Pigeons.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Basic usage (local)</a></li><li><a class="tocitem" href="../unidentifiable-example/">Why parallel tempering (PT)?</a></li><li><a class="tocitem" href="../parallel/">Parallelization</a></li><li><a class="tocitem" href="../mpi/">Distributed usage (MPI)</a></li><li><a class="tocitem" href="../variational/">Variational PT</a></li><li><span class="tocitem">Supported inputs</span><ul><li><a class="tocitem" href="../input-overview/">Inputs overview</a></li><li><a class="tocitem" href="../input-turing/">Turing.jl model</a></li><li><a class="tocitem" href="../input-julia/">Black-box function</a></li><li><a class="tocitem" href="../input-stan/">Stan model</a></li><li><a class="tocitem" href="../input-nonjulian/">Non-julian MCMC</a></li><li><a class="tocitem" href="../input-explorers/">Custom MCMC</a></li></ul></li><li><span class="tocitem">Outputs</span><ul><li><a class="tocitem" href="../output-overview/">Outputs overview</a></li><li><a class="tocitem" href="../output-inferencereport/">Automated reports</a></li><li><a class="tocitem" href="../output-reports/">Standard output</a></li><li><a class="tocitem" href="../output-traces/">Traces</a></li><li><a class="tocitem" href="../output-plotting/">Plots</a></li><li><a class="tocitem" href="../output-normalization/">log(Z)</a></li><li><a class="tocitem" href="../output-numerical/">Numerical</a></li><li><a class="tocitem" href="../output-online/">Online stats</a></li><li><a class="tocitem" href="../output-off-memory/">Off-memory</a></li><li><a class="tocitem" href="../output-pt/">PT diagnostics</a></li><li><a class="tocitem" href="../output-custom-types/">Custom types</a></li><li><a class="tocitem" href="../output-extended/">Extended output</a></li><li><a class="tocitem" href="../output-mpi-postprocessing/">MPI output</a></li></ul></li><li><a class="tocitem" href="../checkpoints/">Checkpoints</a></li><li><a class="tocitem" href="../correctness/">Correctness checks</a></li><li><a class="tocitem" href="../pt/">More on PT</a></li><li class="is-active"><a class="tocitem" href>More on distributed PT</a><ul class="internal"><li><a class="tocitem" href="#Introduction"><span>Introduction</span></a></li><li><a class="tocitem" href="#Overview-of-the-algorithm"><span>Overview of the algorithm</span></a></li><li><a class="tocitem" href="#Splittable-random-streams"><span>Splittable random streams</span></a></li><li><a class="tocitem" href="#Distributed-replicas"><span>Distributed replicas</span></a></li><li><a class="tocitem" href="#Distributed-swaps"><span>Distributed swaps</span></a></li><li><a class="tocitem" href="#Distributed-reduction"><span>Distributed reduction</span></a></li></ul></li><li><a class="tocitem" href="../interfaces/">Interfaces</a></li><li><a class="tocitem" href="../reference/">Reference</a></li><li><a class="tocitem" href="../openings/">Openings</a></li><li><a class="tocitem" href="../about-us/">About Us</a></li><li><a class="tocitem" href="../gsoc/">Google Summer of Code</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>More on distributed PT</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>More on distributed PT</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/Julia-Tempering/Pigeons.jl/blob/main/docs/src/distributed.md#" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="distributed"><a class="docs-heading-anchor" href="#distributed">Distributed and parallel implementation of PT</a><a id="distributed-1"></a><a class="docs-heading-anchor-permalink" href="#distributed" title="Permalink"></a></h1><h2 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h2><p>Pigeons provides an implementation of Distributed PT based on <a href="https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12464">Syed et al., 2021</a>,  Algorithm 5. This page describes our strategies for addressing the challenges of implementing this distributed,  parallelized, and randomized algorithm.</p><div class="admonition is-info"><header class="admonition-header">Note</header><div class="admonition-body"><p>Read this page if you are interested in extending Pigeons or  understanding how it works under the hood.  Reading this page is not required to use Pigeons. Instead, refer to the  <a href="../#index">user guide</a>. </p></div></div><p>In Distributed PT, one or several computers run MCMC simulations in parallel and  communicate with each other to improve MCMC efficiency.  We use the terminology <strong>machine</strong> for one of these computers, or, to be more precise,  <strong><a href="https://en.wikipedia.org/wiki/Process_(computing)">process</a></strong>. In the typical setting, each machine will run one process, since our implementation also supports  the use of several Julia <strong><a href="https://docs.julialang.org/en/v1/manual/multi-threading/">threads</a></strong>.</p><p>Pigeons is designed so that it is suitable in all these scenarios:</p><ol><li>one machine running PT on one thread,</li><li>one machine running PT on several threads,</li><li>several machines running PT, each using one thread, and</li><li>several machines running PT, each using several threads.</li></ol><p>Ensuring code correctness at the intersection of randomized, parallel, and distributed algorithms is a challenge.  To address this challenge, we designed Pigeons based on the following principle:</p><div class="admonition is-info"><header class="admonition-header">Parallelism Invariance</header><div class="admonition-body"><p>The output of Pigeons is invariant to the number of machines and/or threads.</p></div></div><p>In other words, if <span>$X_{m, t}(s)$</span> denotes the output of Pigeons when provided <span>$m$</span> machines, <span>$t$</span> threads  per machine, and random seed <span>$s$</span>, we guarantee that <span>$X_{m, t}(s) = X_{m&#39;, t&#39;}(s)$</span> for all <span>$m&#39;, t&#39;$</span>. </p><p>Without explicitly keeping Parallelism Invariance in mind during software construction,  parallel/distributed implementations of randomized algorithms will  typically only guarantee <span>$E[X_{m, t}] = E[X_{m&#39;, t&#39;}]$</span> for all <span>$m, m&#39;, t, t&#39;$</span>. While equality in distribution is technically  sufficient, the stronger pointwise equality required by Parallelism Invariance makes  debugging and software validation considerably easier.  This is because the developer can first focus on the fully serial randomized algorithm,  and then use it as an easy-to-compare gold-standard reference for parallel/distributed  implementations.  This strategy is used extensively in Pigeons to ensure correctness.  In contrast, testing equality in distribution, while possible (e.g., see  <a href="https://www.jstor.org/stable/27590449#metadata_info_tab_contents">Geweke, 2004</a>), incurs additional  false negatives due to statistical error. </p><p>Two factors tend to cause violations of Parallelism Invariance: </p><ul><li>Global, thread-local and task-local random number generators (the dominant approaches to parallel   random number generators in current languages).</li><li><a href="https://en.wikipedia.org/wiki/Associative_property#:~:text=non%2Dassociative%20magmas.-,Nonassociativity%20of%20floating%20point%20calculation,sized%20values%20are%20joined%20together">Non-associativity of floating point operations</a>. As a result, when several workers    perform <a href="https://en.wikipedia.org/wiki/MapReduce">Distributed reduction</a> of    floating point values, the output of this reduction will be slightly different.    When these reductions are then fed into further random operations, this implies    two randomized algorithms with the same seed but using a different number of workers    will eventually arbitrarily diverge pointwise. </li></ul><p>One focus in the remainder of this page is to describe how our implementation sidesteps  the two above issues while maintaining the same asymptotic runtime complexity.</p><h2 id="Overview-of-the-algorithm"><a class="docs-heading-anchor" href="#Overview-of-the-algorithm">Overview of the algorithm</a><a id="Overview-of-the-algorithm-1"></a><a class="docs-heading-anchor-permalink" href="#Overview-of-the-algorithm" title="Permalink"></a></h2><p>Let us start with a high-level picture of the distributed PT algorithm. </p><p>The high-level code is the function <a href="../reference/#Pigeons.pigeons-Tuple{}"><code>pigeons()</code></a> which is identical to the single-machine algorithm.  A first difference lay in the <a href="../interfaces/#replicas"><code>replicas</code></a> datastructure taking on a different type. Also, as promised the  output is identical despite a vastly different swap logic: this can be checked using the <code>checked_round</code>  argument described in the <a href="../#index">user guide</a>.  A second difference between the execution of <a href="../reference/#Pigeons.pigeons-Tuple{}"><code>pigeons()</code></a> in single vs many machine context is the behaviour  of <a href="../reference/#Pigeons.swap!-Tuple{Any, Any, Any}"><code>swap!</code></a> which is dispatched  based on the type of  <code>replicas</code>. </p><p>In the following, we go over the main building block of  our distributed PT algorithm. </p><h2 id="Splittable-random-streams"><a class="docs-heading-anchor" href="#Splittable-random-streams">Splittable random streams</a><a id="Splittable-random-streams-1"></a><a class="docs-heading-anchor-permalink" href="#Splittable-random-streams" title="Permalink"></a></h2><p>The first building block is a splittable random stream.  To motivate splittable random streams, consider the following example violating Parallelism Invariance.</p><p>Julia uses <em>task-local</em> random number generators, a notion which  is related but distinct from parallelism invariance.  We will now explain the difference between task-local random number  generators and parallelism invariance, and why the latter is more  advantageous for checking correctness of distributed randomized algorithms. </p><p>Consider the following toy example:</p><pre><code class="nohighlight hljs">using Random
import Base.Threads.@threads

println(&quot;Number of threads: $(Threads.nthreads())&quot;)

const n_iters = 10000;
result = zeros(n_iters);
Random.seed!(1);
@threads for i in 1:n_iters
    # in a real problem, do some expensive calculation here...
    result[i] = rand();
end
println(&quot;Result: $(last(result))&quot;)</code></pre><p>When using 8 threads, this outputs:</p><pre><code class="nohighlight hljs">Number of threads: 8
Result: 0.25679999169092793</code></pre><p>Julia guarantees that if we rerun this code, as long as we  are using 8 threads, we will always get the same result,  irrespective of the multi-threading scheduling decisions  implied by the <code>@threads</code>-loop (hence, a step ahead another  concept known as thread-local random number generation, which does not guarantee replicability even for a fixed number of  threads). </p><p>However, when we use a different number of threads (e.g.,  the key example is one thread), the result is different:</p><pre><code class="nohighlight hljs">Number of threads: 1
Result: 0.8785201210435906</code></pre><p>In this simple example above, it is not a big deal, but for our parallel tempering use case, the  distributed version of the algorithm is significantly more complex and  harder to debug compared to the single-threaded one. Hence we take  task-local random number generation one step further, into <strong>parallelism  invariance</strong>, which will guarantee that the output is not only  reproducible with respect to repetitions for a fixed number of threads,  but also for different numbers of threads. </p><p>In our context, a first step to achieve this is to associate one random number generator to each PT chain. To do so, we use the  <a href="https://github.com/UBC-Stat-ML/SplittableRandoms.jl">SplittableRandoms.jl library</a> which allows  us to turn one seed into an arbitrary collection pseudo-independent random number generators.  Since each MPI process holds a subset of the chains, we internally use the  function <a href="../reference/#Pigeons.split_slice-Tuple{UnitRange, Any}"><code>split_slice()</code></a> to  get the random number generators for the slice of replicas held in a given MPI process.</p><h2 id="Distributed-replicas"><a class="docs-heading-anchor" href="#Distributed-replicas">Distributed replicas</a><a id="Distributed-replicas-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-replicas" title="Permalink"></a></h2><p>Calling <a href="../reference/#Pigeons.create_entangled_replicas-Tuple{Inputs, Shared, Any}"><code>create_entangled_replicas()</code></a> will produce a fresh <a href="../reference/#Pigeons.EntangledReplicas"><code>EntangledReplicas</code></a>,  taking care of distributed random seed splitting internally.  An <code>EntangledReplicas</code> contains the list of replicas that are local to the machine, in addition to three data structures allowing distributed communication:  a <a href="../reference/#Pigeons.LoadBalance"><code>LoadBalance</code></a> which keeps track of  how to split work across machines; an <a href="../reference/#Pigeons.Entangler"><code>Entangler</code></a>, which encapsulates MPI calls;  and a <a href="../reference/#Pigeons.PermutedDistributedArray"><code>PermutedDistributedArray</code></a>, which   maps chain indices to replica indices. These datastructures can be obtained using <a href="../reference/#Pigeons.load-Tuple{Any}"><code>load()</code></a>, <a href="../reference/#Pigeons.entangler-Tuple{Any}"><code>entangler()</code></a>, and  <code>replicas.chain_to_replica_global_indices</code> respectively.</p><h2 id="Distributed-swaps"><a class="docs-heading-anchor" href="#Distributed-swaps">Distributed swaps</a><a id="Distributed-swaps-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-swaps" title="Permalink"></a></h2><p>To perform distributed swaps, <a href="../reference/#Pigeons.swap!-Tuple{Any, Any, Any}"><code>swap!()</code></a> proceeds as follows:</p><ol><li>Use the <a href="../interfaces/#swap_graph"><code>swap_graph</code></a> to determine swapping partner chains,</li><li>translate partner chains into partner replicas (global indices) using  <code>replicas.chain_to_replica_global_indices</code>,</li><li>compute <a href="../reference/#Pigeons.swap_stat-Tuple{Any, Pigeons.Replica, Int64}"><code>swap_stat()</code></a> for local chains, and use   <a href="../reference/#Pigeons.transmit-Union{Tuple{T}, Tuple{Pigeons.Entangler, AbstractVector{T}, AbstractVector{Int64}}} where T"><code>transmit()</code></a> to obtain partner swap stats,</li><li>use <a href="../reference/#Pigeons.swap_decision-Tuple{Any, Int64, Any, Int64, Any}"><code>swap_decision()</code></a> to decide if each pair should swap, and </li><li>update the <code>replicas.chain_to_replica_global_indices</code> datastructure. </li></ol><h2 id="Distributed-reduction"><a class="docs-heading-anchor" href="#Distributed-reduction">Distributed reduction</a><a id="Distributed-reduction-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-reduction" title="Permalink"></a></h2><p>After each round of PT, the workers need to exchange richer messages compared to the information exchanged in the swaps.  These richer messages include swap acceptance probabilities,  statistics to adapt a variational reference, etc. </p><p>This part of the communication is performed using <a href="../reference/#Pigeons.reduce_recorders!-Tuple{Any, EntangledReplicas}"><code>reduce_recorders!()</code></a> which  in turn calls <a href="../reference/#Pigeons.all_reduce_deterministically-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, Pigeons.Entangler}} where T"><code>all_reduce_deterministically()</code></a> with the appropriate   merging operations. See <a href="../reference/#Pigeons.reduce_recorders!-Tuple{Any, EntangledReplicas}"><code>reduce_recorders!()</code></a> and  <a href="../reference/#Pigeons.all_reduce_deterministically-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, Pigeons.Entangler}} where T"><code>all_reduce_deterministically()</code></a> for more information on how  our implementation preserves Parallelism Invariance, while maintaining the logarithmic runtime of binary-tree based  collective operations. (More precisely, <code>all_reduce_deterministically()</code> runs in time <span>$\log(N)$</span>  when each machine holds a single chain.)</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../pt/">« More on PT</a><a class="docs-footer-nextpage" href="../interfaces/">Interfaces »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Friday 20 December 2024 18:06">Friday 20 December 2024</span>. Using Julia version 1.10.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
