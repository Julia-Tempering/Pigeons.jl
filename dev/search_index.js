var documenterSearchIndex = {"docs":
[{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Descriptions of informal interfaces (see Pigeons.@informal to see how this page  was generated).","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#log_potential","page":"Interfaces","title":"log_potential","text":"","category":"section"},{"location":".interfaces/#Description","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.log_potential","category":"page"},{"location":".interfaces/#Pigeons.log_potential","page":"Interfaces","title":"Pigeons.log_potential","text":"A log_potential encodes a probability distribution, where only the  un-normalized probability density function is known. \n\nTo make MyType conforms this informal interface, implement \n\n(log_potential::MyType)(x)\n\nwhich should return the log of the un-normalized density.\n\nFor example, we provide this behaviour for any distribution  in Distributions.jl. \n\n\n\n\n\n","category":"constant"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#log_potentials","page":"Interfaces","title":"log_potentials","text":"","category":"section"},{"location":".interfaces/#Description-2","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.log_potentials","category":"page"},{"location":".interfaces/#Pigeons.log_potentials","page":"Interfaces","title":"Pigeons.log_potentials","text":"An encoding of a discrete set of probability distributions, where only the un-normalized  probability density functions are known.  Each distribution is allowed to have a different normalization constant. \n\nFor example, we provide this behaviour for any Vector containing log_potential's. \n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.log_unnormalized_ratio()","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#pair_swapper","page":"Interfaces","title":"pair_swapper","text":"","category":"section"},{"location":".interfaces/#Description-3","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.pair_swapper","category":"page"},{"location":".interfaces/#Pigeons.pair_swapper","page":"Interfaces","title":"Pigeons.pair_swapper","text":"Informs swap!() of how to perform a swap between a given pair of chains.\n\nThis is done in two steps:\n\nUse swap_stat() to extract sufficient statistics needed to make a swap decision. \nGiven these statistics for the two chains, swap_decision() then perform the swap.\n\nThe rationale for breaking this down into two steps is that in a distributed swap context, swap!() will take care of transmitting the sufficient  statistics over the network if necessary.\n\nThe function record_swap_stats!() is used to record information about swapping,  in particular mean swap acceptance probabilities.\n\nA default implementation of all of pair_swapper's methods is provided,  where the pair_swapper is assumed to follow the log_potentials interface.\n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract-2","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.swap_stat()\nPigeons.record_swap_stats!()\nPigeons.swap_decision()","category":"page"},{"location":".interfaces/#Examples-of-functions-providing-instances-of-this-interface","page":"Interfaces","title":"Examples of functions providing instances of this interface","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.TestSwapper()","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#path","page":"Interfaces","title":"path","text":"","category":"section"},{"location":".interfaces/#Description-4","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.path","category":"page"},{"location":".interfaces/#Pigeons.path","page":"Interfaces","title":"Pigeons.path","text":"A continuum of log_potential's interpolating between two end-points. More precisely, a mapping from [0, 1] to the space of probability distributions.\n\nThe main use of this interface is to pass it to discretize().\n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract-3","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.interpolate()","category":"page"},{"location":".interfaces/#Examples-of-functions-providing-instances-of-this-interface-2","page":"Interfaces","title":"Examples of functions providing instances of this interface","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.create_path()","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#recorder","page":"Interfaces","title":"recorder","text":"","category":"section"},{"location":".interfaces/#Description-5","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.recorder","category":"page"},{"location":".interfaces/#Pigeons.recorder","page":"Interfaces","title":"Pigeons.recorder","text":"Accumulate a specific type of statistic, for example  by keeping constant size sufficient statistics  (via OnlineStat, which conforms this interface),  storing samples to a file, etc.  See also recorders.\n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract-4","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.record!()\nPigeons.combine()","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#recorders","page":"Interfaces","title":"recorders","text":"","category":"section"},{"location":".interfaces/#Description-6","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.recorders","category":"page"},{"location":".interfaces/#Pigeons.recorders","page":"Interfaces","title":"Pigeons.recorders","text":"A NamedTuple containing several recorder's.  Each recorder is responsible for a type of statistic to be  accumulated. \n\nThe keyset of the NamedTuple controls which types of  statistics to accumulate (we refer to each element in  this keyset as a recorder_key). By default, only those  with constant memory requirement are selected, the user  can select more expensive ones by enlarging that keyset.\n\nDuring PT execution, each recorders object keep track of only the  statistics for one replica (for thread safety and/or  distribution purpose). After a PT round, use reduced_recorders() to do  a reduction before  accessing statistic values. \n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract-5","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.record_if_requested!()","category":"page"},{"location":".interfaces/#Examples-of-functions-providing-instances-of-this-interface-3","page":"Interfaces","title":"Examples of functions providing instances of this interface","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.custom_recorders()\nPigeons.default_recorders()","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#replicas","page":"Interfaces","title":"replicas","text":"","category":"section"},{"location":".interfaces/#Description-7","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.replicas","category":"page"},{"location":".interfaces/#Pigeons.replicas","page":"Interfaces","title":"Pigeons.replicas","text":"Stores the process' replicas.  Since we provide MPI implementations, do not assume that this will contain all the replicas, as  others can be located in other processes/machines\n\nImplementations provided\n\nEntangledReplicas: using an MPI-based implementation\nVector{Replica}: for the single process case (above can handle that case, but the array based implementation is non-allocating)\n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract-6","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.swap!()\nPigeons.locals()\nPigeons.load()\nPigeons.communicator()\nPigeons.entangler()","category":"page"},{"location":".interfaces/#Examples-of-functions-providing-instances-of-this-interface-4","page":"Interfaces","title":"Examples of functions providing instances of this interface","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.create_entangled_replicas()\nPigeons.create_vector_replicas()","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#state_initializer","page":"Interfaces","title":"state_initializer","text":"","category":"section"},{"location":".interfaces/#Description-8","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.state_initializer","category":"page"},{"location":".interfaces/#Pigeons.state_initializer","page":"Interfaces","title":"Pigeons.state_initializer","text":"Determine how to initialize the states in the replicas.  Implementations include Ref(my_state), to signal all replicas will  be initalized to my_state, or a Vector(...) for chain-specific  initializations.\n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract-7","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.initialization()","category":"page"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"","category":"page"},{"location":".interfaces/#swap_graph","page":"Interfaces","title":"swap_graph","text":"","category":"section"},{"location":".interfaces/#Description-9","page":"Interfaces","title":"Description","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.swap_graph","category":"page"},{"location":".interfaces/#Pigeons.swap_graph","page":"Interfaces","title":"Pigeons.swap_graph","text":"Informs swap!() about which chain will interact with which.\n\nCanonical example is the standard Odd and Even swap, extension point for e.g. \n\nparallel parallel tempering\nvariational methods with more than 2 legs,\nPT algorithms dealing with more than one target simultaneously for the purpose of model selection. \n\n\n\n\n\n","category":"constant"},{"location":".interfaces/#Contract-8","page":"Interfaces","title":"Contract","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.partner_chain()\nPigeons.reference_chains()\nPigeons.target_chains()","category":"page"},{"location":".interfaces/#Examples-of-functions-providing-instances-of-this-interface-5","page":"Interfaces","title":"Examples of functions providing instances of this interface","text":"","category":"section"},{"location":".interfaces/","page":"Interfaces","title":"Interfaces","text":"Pigeons.deo()","category":"page"},{"location":"reference/","page":"Reference","title":"Reference","text":"CurrentModule = Pigeons","category":"page"},{"location":"reference/#Index","page":"Reference","title":"Index","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"","category":"page"},{"location":"reference/#Types-and-functions","page":"Reference","title":"Types and functions","text":"","category":"section"},{"location":"reference/","page":"Reference","title":"Reference","text":"Modules = [Pigeons]\nFilter = t -> typeof(t) !== Pigeons.InformalInterfaceSpec","category":"page"},{"location":"reference/#Pigeons.EntangledReplicas","page":"Reference","title":"Pigeons.EntangledReplicas","text":"An implementation of replicas for distributed PT.  Contains:\n\nlocals\n\n: The subset of replicas hosted in this process\n\nchain_to_replica_global_indices\n\n: A specialized distributed array that     maps chain indices to replica indices (global indices).     This corresponds to the mapping boldsymbolj in line 2 of     Algorithm 5 in Syed et al, 2021.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.Entangler","page":"Reference","title":"Pigeons.Entangler","text":"Assume all the MPI processes linked by this communicator  will all call the key operations listed below the same number of times  in their lifetime, at logically related occasions (e.g. a set  number of times per iterations for algorithms running the  same number of iterations). We call these 'occasions' a micro-iteration.\n\nThis datastructure keeps track internally of appropriate unique  tags to coordinate the communication between MPI processes  without having to do any explicit synchnonization. \n\nThis struct contains:\n\ncommunicator\n\n: An MPI Comm object (or nothing if a single process is involved).\n\nload\n\n: How a set of tasks or \"global indices\" are distributed across processes.\n\ncurrent_received_bits\n\n: An internal datastructure used during MPI calls.\n\nn_transmits\n\n: The current micro-iteration.\n\nThe key operations supported:\n\ntransmit() and transmit!(): encapsulates    pairwise communications in which each MPI process is holding     a Vector, the elements of which is to be permuted across the processes.\nall_reduce_deterministically and reduce_deterministically,    to perform MPI collective reduction while maintaining the    Parallelism Invariance property.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.LoadBalance","page":"Reference","title":"Pigeons.LoadBalance","text":"Split a list of indices across processes.  These indices are denoted 1 2  N. They are usually some kind of task,  for example in the context of parallel tempering,  two kinds of tasks arise:\n\nin replicas.state, task i consists in keeping track of the state of    replica i.\nin replicas.chain_to_replica_global_indices, task i consists in    storing which replica index corresponds to chain i.\n\nOne such task index is called a global_index. \n\nLoadBalance splits the global indices among n_processes. LoadBalance  is constructed so that the difference in the number of global indices  a process is responsible of (its \"load\")  is at most one.\n\nA LoadBalance contains:\n\nmy_process_index\n\n: A unique index for this process.     We use 1-indexed,     i.e. hide MPI's 0-indexed ranks.\n\nn_processes\n\n: Total number of processes involved.\n\nn_global_indices\n\n: The total number of global indices shared between all the processes.\n\nThe set {1, 2, .., load()} is called a set of local indices.  A local index indexes a slice in {1, 2, ..., n_global_indices}.  Collectively over the n_processes, these slices form a partition of  the global indices.\n\nKey functions to utilize a LoadBalance struct:\n\nmy_global_indices()\nfind_process()\nfind_local_index()\nmy_load()\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.PermutedDistributedArray","page":"Reference","title":"Pigeons.PermutedDistributedArray","text":"A distributed array making special assumptions on how  it will be accessed and written to.  The indices of this distributed array correspond to the  notion of \"global indices\" defined in LoadBalance.  Several MPI processes cooperate, each processing storing  data for a slice of this distributed array. \n\nWe make the following assumptions:\n\nEach MPI process will set/get    entries the same number of times in their lifetime, at    logically related episodes (e.g. a set    number of times per iterations for algorithms running the    same number of iterations).    These episodes are called micro-iteration as in Entangler,    which this datastructure is built on.\nMoreover, at each time all process perform a get or a set,    we assume that each global index is manipulated by exactly one    process (i.e. an implicit permutation of the global indices).\n\nWe use these assumptions to achieve  read/write costs that are  near-constant in the number of machines participating. \n\nThis struct contains:\n\nlocal_data\n\n: The slice of the distributed array maintained by this MPI process.\n\nentangler\n\n: An Entangler used to coordinate communication.\n\nThe operations supported are:\n\npermuted_get()\npermuted_set!()\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.Replica","page":"Reference","title":"Pigeons.Replica","text":"One of the N components that forms the state maintained by a PT algorithm. A Replica contains:\n\nstate\n\n:  Configuration in the state space.\n\nchain\n\n:  The index of the distribution currently associated with this replica, modified during swaps.\n\nrng\n\n:  Random operations involving this state should use only this random number generator.\n\nrecorders\n\n: Records statistics. Each replica carries its own for thread safety/distribution, to be reduced to access.\n\nreplica_index\n\n: A global id associated with this replica.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.ScaledPrecisionNormalPath","page":"Reference","title":"Pigeons.ScaledPrecisionNormalPath","text":"A path of zero-mean normals for testing; contains:\n\nprecision0\n\n: Precision parameter of the reference.\n\nprecision1\n\n: Precision parameter of the target.\n\ndim\n\n: Dimensionality.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.ScaledPrecisionNormalPath-Tuple{Int64}","page":"Reference","title":"Pigeons.ScaledPrecisionNormalPath","text":"ScaledPrecisionNormalPath(\n    dim::Int64\n) -> ScaledPrecisionNormalPath\n\n\nToy path for testing: see section I.4.1 in Syed et al 2021. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.Schedule","page":"Reference","title":"Pigeons.Schedule","text":"A partition of [0, 1] encoded by monotonically increasing grid points  starting at zero and ending at one.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.Schedule-Tuple{Int64, Any}","page":"Reference","title":"Pigeons.Schedule","text":"Schedule(\n    n_chains::Int64,\n    cumulativebarrier\n) -> Schedule{Vector{Float64}}\n\n\nCreate a Schedule with n_chains grid points computed using Algorithm 2 in  Syed et al, 2021. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.Schedule-Tuple{Int64}","page":"Reference","title":"Pigeons.Schedule","text":"Schedule(\n    n_chains::Int64\n) -> Schedule{StepRangeLen{Float64, Base.TwicePrecision{Float64}, Base.TwicePrecision{Float64}, Int64}}\n\n\nCreate a Schedule with n_chains equally spaced grid points.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.Summary","page":"Reference","title":"Pigeons.Summary","text":"Store summary output from PT\n\nStruct that contains summary obtained output after running PT. For continuous and discrete variables, the mean and variance from the  target chain are stored. TODO: Choose more appropriate summary statistics or discrete variables.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.SwapStat","page":"Reference","title":"Pigeons.SwapStat","text":"Default statistics exchanged by a pair of chains in the process of proposing a swap:\n\nlog_ratio\nuniform\n\nSee swap_stat()\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.TestSwapper","page":"Reference","title":"Pigeons.TestSwapper","text":"For testing/benchmarking purpose, a simple  pair_swapper where all swaps have equal  acceptance probability. \n\nCould also be used to warm-start swap connections  during exploration phase by setting that  constant probability to zero.  \n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.TranslatedNormalPath","page":"Reference","title":"Pigeons.TranslatedNormalPath","text":"mean\n\n: Path between a MVN with mean zero at one end point and given mean at the other.\n\nToy path for testing.\n\n\n\n\n\n","category":"type"},{"location":"reference/#Pigeons.NRPT-Tuple{Any, Any, Array{Vector{T}, 1} where T<:Real, Int64, Int64}","page":"Reference","title":"Pigeons.NRPT","text":"NRPT(V_0, V_1, initial_state, ntotal, N)\n\nNon-reversible parallel tempering (NRPT).\n\nArguments\n\npotential: Function with three arguments (x, η, params) that returns a 'double'.  'x' is the point at which the log-density V0(x; params=params) * η[1] + V1(x) * η[2] is evaluated,  where V0 is the negative log density of the reference and V1 is the negative  log density of the target.\ninitial_state: Matrix of initial states for all N+1 chains. Dimensions: (N+1) x (dim_x).\nntotal: Total number of scans/iterations.\nN: The total number of chains is N+1.\noptimreference: Whether the reference distribution is to be optimized.\nmaxround: Maximum number of rounds for tuning.\nfulltrajectory: Controls whether to keep track of all 'states', 'indices', 'energies', and 'lifts'.\nϕ: (Partially removed. Useful for constructing non-linear paths.)\nresolution: Resolution of the output for the estimates of the local communication barrier. \nprior_sampler: User may supply an efficient sampler that can obtain   samples from the prior / original reference distribution.\noptimreference_start: On which tuning round to start optimizing the reference distribution.\nfull_covariance: Controls whether to use a mean-field approximation for the modified   reference (false) or a full covariance matrix (true)\nwinsorize: Whether or not to use a winsorized/trimmed mean when estimating \n\nthe parameters of the variational reference\n\ntwo_references: Whether to run two PT chains in parallel with two different references:   prior and variational reference. Note that with this setting there are 2*(N+1) chains in total.\nmodref_means_start: Starting values for modref_means\nmodref_stds_start: Starting values for modref_stds\nn_explore: Number of exploration steps to take before considering a communication swap\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.acceptanceprobability-Tuple{Any, Any, Any}","page":"Reference","title":"Pigeons.acceptanceprobability","text":"acceptanceprobability(newenergy, newenergy1, newenergy2)\n\nCompute acceptance probabilities for communication moves.  newenergy inputs are lists of numbers. \n\nArguments\n\nnewenergy: -log([πβ0(x^0), πβ1(x^1), ..., πβN(x^N)]) : length N+1\nnewenergy1: -log([πβ0(x^1), πβ1(x^2), ..., πβ{N-1}(x^N)]) : length N\nnewenergy2: -log([πβ1(x^0), πβ2(x^1), ..., πβN(x^{N-1})]) : length N\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.all_reduce_deterministically-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, Entangler}} where T","page":"Reference","title":"Pigeons.all_reduce_deterministically","text":"Same as reduce_deterministically() except that the result at the root of the  tree is then broadcast to all machines so that the output of all_reduce_deterministically()  is the root of the reduction tree for all MPI processes involved. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.analytic_cumulativebarrier-Tuple{ScaledPrecisionNormalPath}","page":"Reference","title":"Pigeons.analytic_cumulativebarrier","text":"analytic_cumulativebarrier(\n    path::ScaledPrecisionNormalPath\n) -> Pigeons.var\"#cumulativebarrier#30\"{ScaledPrecisionNormalPath, Float64}\n\n\nKnown cumulative barrier used for testing,  from Predescu et al., 2003.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.combine-Tuple{Any, Any}","page":"Reference","title":"Pigeons.combine","text":"combine(recorder1, recorder2) -> Any\n\n\nCombine the two provided recorder objects. \n\nBy default, call Base.merge().\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.communicationbarrier-Tuple{AbstractVector, AbstractVector}","page":"Reference","title":"Pigeons.communicationbarrier","text":"communicationbarrier(\n    rejection::AbstractVector,\n    schedule::AbstractVector\n) -> NamedTuple{(:localbarrier, :cumulativebarrier, :globalbarrier), _A} where _A<:Tuple{Pigeons.var\"#localbarrier#34\", Pigeons.var\"#cumulativebarrier#33\", Any}\n\n\nCompute the local communication barrier and cumulative barrier functions from the  rejection rates and the current annealing schedule. The estimation of the barriers  is based on Fritsch-Carlson monotonic interpolation.\n\nReturns a NamedTuple with fields:\n\nlocalbarrier\ncumulativebarrier\nglobalbarrier\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.communicator-Tuple{Any}","page":"Reference","title":"Pigeons.communicator","text":"communicator(replicas)\n\n\nReturn the replicas's MPI.Comm or nothing if no MPI needed\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.computeetas-Tuple{Any, Any}","page":"Reference","title":"Pigeons.computeetas","text":"computeetas(ϕ, β)\n\nCompute the etas matrix given ϕ, which is an Array(K - 1, 2) containing  knot parameters, and β, a vector of N+1 schedules. For linear paths,  the function returns an (N+1)x2 matrix with entries 1-β in the first column  and β in the second column. (This function is useful for those wishing to consider non-linear paths. However, full support is provided only for linear paths at  the moment.) \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.create_entangled_replicas","page":"Reference","title":"Pigeons.create_entangled_replicas","text":"create_entangled_replicas(\n    n_chains::Int64,\n    state_initializer,\n    rng::SplittableRandoms.SplittableRandom\n) -> EntangledReplicas\ncreate_entangled_replicas(\n    n_chains::Int64,\n    state_initializer,\n    rng::SplittableRandoms.SplittableRandom,\n    useMPI::Bool\n) -> EntangledReplicas\ncreate_entangled_replicas(\n    n_chains::Int64,\n    state_initializer,\n    rng::SplittableRandoms.SplittableRandom,\n    useMPI::Bool,\n    recorder_keys::Set{Symbol}\n) -> EntangledReplicas\n\n\nCreate distributed replicas. The argument useMPI = false is only for debugging purpose. See also state_initializer. \n\n\n\n\n\n","category":"function"},{"location":"reference/#Pigeons.create_path","page":"Reference","title":"Pigeons.create_path","text":"create_path(ref, target) -> Any\ncreate_path(ref, target, interpolator) -> Any\n\n\nGiven a reference log_potential and a target log_potential,  return a path interpolating between the two. \n\nBy default, the interpolator is a LinearInterpolator, i.e.  standard annealing.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Pigeons.create_vector_replicas","page":"Reference","title":"Pigeons.create_vector_replicas","text":"create_vector_replicas(\n    n_chains::Int64,\n    state_initializer,\n    rng::SplittableRandoms.SplittableRandom\n) -> Any\ncreate_vector_replicas(\n    n_chains::Int64,\n    state_initializer,\n    rng::SplittableRandoms.SplittableRandom,\n    recorder_keys::Set{Symbol}\n) -> Any\n\n\nCreate replicas when distributed computing is not needed.  See also state_initializer.\n\n\n\n\n\n","category":"function"},{"location":"reference/#Pigeons.custom_recorders-Tuple{Set{Symbol}}","page":"Reference","title":"Pigeons.custom_recorders","text":"custom_recorders(\n    recorder_keys::Set{Symbol}\n) -> Union{NamedTuple{(:swap_acceptance_pr,), Tuple{OnlineStatsBase.GroupBy{Tuple{Int64, Int64}, Number, OnlineStatsBase.Mean{Float64, OnlineStatsBase.EqualWeight}}}}, NamedTuple{(:swap_acceptance_pr, :index_process), Tuple{OnlineStatsBase.GroupBy{Tuple{Int64, Int64}, Number, OnlineStatsBase.Mean{Float64, OnlineStatsBase.EqualWeight}}, Dict{Int64, Vector{Int64}}}}}\n\n\nThis returns default_recorders() plus those  provided in the recorder_keys. \n\nSee also recorder_keys.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.default_recorders-Tuple{}","page":"Reference","title":"Pigeons.default_recorders","text":"default_recorders(\n\n) -> NamedTuple{(:swap_acceptance_pr,), Tuple{OnlineStatsBase.GroupBy{Tuple{Int64, Int64}, Number, OnlineStatsBase.Mean{Float64, OnlineStatsBase.EqualWeight}}}}\n\n\nBasic, constant-memory recorders.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.deo-Tuple{Any, Any, Any, Any, Any, Any, Int64, Int64, Int64, Any, Any, Any, Any, Bool, Any, Int64}","page":"Reference","title":"Pigeons.deo","text":"deo(potential, initial_state, initial_index, initial_lift, schedule, ϕ, \n    nscan, N, resolution, optimreference_round, modref_means, modref_stds, modref_covs, \n    full_covariance, prior_sampler, n_explore)\n\nDeterministic even-odd parallel tempering (DEO/NRPT).\n\nArguments\n\npotential: Function as in NRPT, but with only two arguments: x and η\ninitial_state: Starting state, as in NRPT. Input is of size: N+1 [ dim_x ]\ninitial_index: Starting indices\ninitial_lift: Starting lift\nschedule: Annealing schedule\nϕ: As in NRPT\nnscan: Number of scans to use\nN: As in NRPT\nresolution: As in NRPT\noptimreference_round: As in NRPT\nmodref_means\nmodref_stds\nprior_sample\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.deo-Tuple{Int64, Int64}","page":"Reference","title":"Pigeons.deo","text":"deo(\n    n_chains::Int64,\n    current_iteration::Int64\n) -> Pigeons.OddEven\n\n\nImplements the Deterministic Even Odd (DEO) scheme proposed in Okabe, 2001 and analyzed in Syed et al., 2021.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.deoscan-NTuple{15, Any}","page":"Reference","title":"Pigeons.deoscan","text":"deoscan(potential, state, index, lift, etas, n, N, kernels, \n    optimreference_round, modref_means, modref_stds, modref_covs, full_covariance, \n    prior_sampler, n_explore)\n\nPerform one DEO scan (local exploration + communication). Arguments are  similar to those for deo(). Note that state is the state from the one previous  scan, which is of size N+1[dim_x].\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.discretize-Tuple{Any, Schedule}","page":"Reference","title":"Pigeons.discretize","text":"discretize(path, betas::Schedule) -> Any\n\n\nCreate log_potentials from a path by interpolating the  path at each grid point specified in the Schedule.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.entangler-Tuple{Any}","page":"Reference","title":"Pigeons.entangler","text":"entangler(replicas) -> Entangler\n\n\nReturn the replicas's Entangler (possibly a no-communication Entangler if a single process is involved)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.find_global_index-Tuple{Pigeons.LoadBalance, Int64}","page":"Reference","title":"Pigeons.find_global_index","text":"find_global_index(\n    lb::Pigeons.LoadBalance,\n    local_idx::Int64\n) -> Int64\n\n\nFind the global index corresponding to the given local_index. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.find_local_index-Tuple{Pigeons.LoadBalance, Int64}","page":"Reference","title":"Pigeons.find_local_index","text":"find_local_index(\n    lb::Pigeons.LoadBalance,\n    global_idx::Int64\n) -> Int64\n\n\nFind the local index corresponding to the given global_index.  Assumes the given global_index is one of this process'. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.find_process-Tuple{Pigeons.LoadBalance, Int64}","page":"Reference","title":"Pigeons.find_process","text":"find_process(\n    lb::Pigeons.LoadBalance,\n    global_idx::Int64\n) -> Int64\n\n\nFind the process id (1-indexed) responsible for the given global_idx. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.index_process_plot-Tuple{Any}","page":"Reference","title":"Pigeons.index_process_plot","text":"index_process_plot(recorders) -> Plots.Plot\n\n\nGiven a recorders, create an index process plot.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.informal_doc-Tuple{Any, Module}","page":"Reference","title":"Pigeons.informal_doc","text":"informal_doc(doc_dir, mod::Module) -> String\n\n\nGenerate informal interface documentation, e.g.: \n\nmakedocs(;\n    ...\n    pages=[\n        \"Home\" => \"index.md\", \n        \"Interfaces\" => informal_doc(@__DIR__, MyModuleName),\n        ...\n    ]\n)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.initialization-Tuple{Any, SplittableRandoms.SplittableRandom, Int64}","page":"Reference","title":"Pigeons.initialization","text":"initialization(\n    state_initializer,\n    rng::SplittableRandoms.SplittableRandom,\n    chain::Int64\n) -> Any\n\n\nDetermine state_initializer's initialization for the given chain.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.interpolate-Tuple{Any, Any}","page":"Reference","title":"Pigeons.interpolate","text":"interpolate(path, beta) -> Pigeons.InterpolatedLogPotential\n\n\nReturns the log_potential at point beta in the path\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.load-Tuple{Any}","page":"Reference","title":"Pigeons.load","text":"load(replicas) -> Pigeons.LoadBalance\n\n\nReturn the replicas's LoadBalance (possibly single_process_load)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.local_exploration-NTuple{9, Any}","page":"Reference","title":"Pigeons.local_exploration","text":"local_exploration(states, kernels, optimreference_round, modref_means, modref_stds, \n    modref_covs, full_covariance, prior_sampler, n_explore)\n    chainacceptance = Vector{Int64}(undef, length(states))\n\nPerform one local exploration move. state is the state from the one  previous scan, which is of size N+1[dim_x].\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.locals-Tuple{Any}","page":"Reference","title":"Pigeons.locals","text":"locals(replicas) -> Vector\n\n\nReturn the replica's that are stored in this machine\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.log_unnormalized_ratio-Tuple{AbstractVector, Int64, Int64, Any}","page":"Reference","title":"Pigeons.log_unnormalized_ratio","text":"log_unnormalized_ratio(\n    log_potentials::AbstractVector,\n    numerator::Int64,\n    denominator::Int64,\n    state\n) -> Any\n\n\nAssumes the input log_potentials is a vector where each element is a log_potential.\n\nThis default implementation is sufficient in most cases, but in less standard scenarios, e.g. where the  state space is infinite dimensional, this can be overridden. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.log_unnormalized_ratio-Tuple{Any, Int64, Int64, Any}","page":"Reference","title":"Pigeons.log_unnormalized_ratio","text":"log_unnormalized_ratio(\n    log_potentials,\n    numerator::Int64,\n    denominator::Int64,\n    state\n) -> Any\n\n\nThe argument numerator selects one distribution pi_i from the collection log_potentials,  and similarly denominator selects pi_j. Let x denote the input state. The ratio:\n\nf(x) = fractextdpi_itextdpi_j(x)\n\nmay only be known up to a normalization constant which can depend on i and j but  not x, g(x) = C_ij f(x).\n\nThis function should return log g evaluated at state.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.lognormalizingconstant-Tuple{Any, Any}","page":"Reference","title":"Pigeons.lognormalizingconstant","text":"lognormalizingconstant(energies, schedule)\n\nCompute an estimate of the log normalizing constant given a vector of  energies and the corresponding annealing schedule.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.my_global_indices-Tuple{Pigeons.LoadBalance}","page":"Reference","title":"Pigeons.my_global_indices","text":"my_global_indices(\n    lb::Pigeons.LoadBalance\n) -> UnitRange{Int64}\n\n\nThe slice of lb.global_indices this process is reponsible of.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.my_load-Tuple{Pigeons.LoadBalance}","page":"Reference","title":"Pigeons.my_load","text":"my_load(lb::Pigeons.LoadBalance) -> Int64\n\n\nReturn the number of indices (task) this process is responsible of. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.n_chains_global-Tuple{Any}","page":"Reference","title":"Pigeons.n_chains_global","text":"n_chains_global(replicas) -> Int64\n\n\nGiven a replicas, return the total number of chains across all processes.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.one_per_host-Tuple{Any}","page":"Reference","title":"Pigeons.one_per_host","text":"For benchmarking purpose: subset the communicator so that at most one MPI process runs      in each machine.\n\nDivision is done so that original rank 0 is always included.\n\nReturn the new communicator or nothing if this machine is not in the subset. \n\nSee also '-s' option in mpi-run\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.partner_chain-Tuple{Any, Int64}","page":"Reference","title":"Pigeons.partner_chain","text":"partner_chain(swap_graph, chain::Int64) -> Int64\n\n\nFor a given swap_graph and input chain index, what chain will it interact with at the current iteration? Convention: if a chain is not interacting, return its index.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.permuted_get-Union{Tuple{T}, Tuple{PermutedDistributedArray{T}, AbstractVector{Int64}}} where T","page":"Reference","title":"Pigeons.permuted_get","text":"permuted_get(\n    p::PermutedDistributedArray{T},\n    indices::AbstractVector{Int64}\n) -> Vector\n\n\nRetreive the values for the given indices, using MPI communication when needed. \n\nWe make the following assumptions:\n\nlength(indices) == my_load(p.entangler.load)\nthe indices across all participating processes form a permutation of the global indices. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.permuted_set!-Union{Tuple{T}, Tuple{PermutedDistributedArray{T}, AbstractVector{T}, AbstractVector{T}}} where T","page":"Reference","title":"Pigeons.permuted_set!","text":"permuted_set!(\n    p::PermutedDistributedArray{T},\n    indices::AbstractArray{T, 1},\n    new_values::AbstractArray{T, 1}\n)\n\n\nSet the values for the given indices to the given new_values, using MPI communication when needed. \n\nWe make the same assumptions as in permuted_get().\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.print-Tuple{Pigeons.Summary}","page":"Reference","title":"Pigeons.print","text":"Print Summary structs\n\nNeatly prints the summary statistics for each variable after running PT.  s is a Summary struct. TODO: Make this output much nicer :)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.record!-Tuple{Any, Any}","page":"Reference","title":"Pigeons.record!","text":"record!(recorder, value) -> Any\n\n\nAdd value to the statistics accumulated by recorder.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.record!-Tuple{OnlineStatsBase.OnlineStat, Any}","page":"Reference","title":"Pigeons.record!","text":"record!(recorder::OnlineStatsBase.OnlineStat, value) -> Any\n\n\nForwards to OnlineStats' fit!\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.record!-Union{Tuple{V}, Tuple{K}, Tuple{Dict{K, Vector{V}}, Tuple{K, V}}} where {K, V}","page":"Reference","title":"Pigeons.record!","text":"record!(\n    recorder::Dict{K, Array{V, 1}},\n    value::Tuple{K, V}\n) -> Vector\n\n\nGiven a value, a pair (a, b), and a Dict{K, Vector{V}} backed  recorder,  append b to the vector corresponding to a, inserting an empty  vector into the dictionary first if needed.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.record_if_requested!-Tuple{NamedTuple, Any, Any}","page":"Reference","title":"Pigeons.record_if_requested!","text":"record_if_requested!(\n    recorders::NamedTuple,\n    recorder_key,\n    value\n) -> Any\n\n\nIf the recorders contains the given recorder_key,  send the value to the recorder corresponding to the  recorder_key. Otherwise, do nothing.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.record_swap_stats!-Tuple{Any, Any, Int64, Any, Int64, Any}","page":"Reference","title":"Pigeons.record_swap_stats!","text":"record_swap_stats!(\n    pair_swapper,\n    recorders,\n    chain1::Int64,\n    stat1,\n    chain2::Int64,\n    stat2\n)\n\n\nGiven a pair_swapper, a recorders, the provided chain indices, and  the sufficient statistics computed by swap_stat(), record statistics. \n\nTo avoid accumulating twice the same statistic with (chain1, chain2) and  (chain2, chain2), swap!() only calls this for the pair with chain1 < chain2.\n\nBy default, the following are accumulated:\n\nthe swap acceptance probability.\nTODO: stepping stone statistics.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.record_swap_stats!-Tuple{Pigeons.TestSwapper, Any, Int64, Any, Int64, Any}","page":"Reference","title":"Pigeons.record_swap_stats!","text":"record_swap_stats!(\n    swapper::Pigeons.TestSwapper,\n    recorder,\n    chain1::Int64,\n    stat1,\n    chain2::Int64,\n    stat2\n)\n\n\nSee TestSwapper.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.recorder_keys-Tuple{Vararg{Symbol}}","page":"Reference","title":"Pigeons.recorder_keys","text":"recorder_keys(args::Symbol...) -> Set\n\n\nSome statistics may induce memory requirements growing in  the number of iterations. Use this to select which ones,  if any to pass to custom_recorders(). E.g.: recorder_keys() or recorder_keys(:index_process).\n\nChoices include (each specifying if it is included  in default_recorders()):\n\n:swap_acceptance_pr: maintain swap acceptance probabilities,   a GroupBy(Tuple{Int, Int}, Mean()) object   (included by default);\n:index_process: keep, for each replica, the list of    chains visited (not included by default), a    Dict{Int, Vector{Int}} object.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.reduce_deterministically-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, Entangler}} where T","page":"Reference","title":"Pigeons.reduce_deterministically","text":"reduce_deterministically(\n    operation,\n    source_data::AbstractArray{T, 1},\n    e::Entangler\n) -> Any\n\n\nPerform a binary reduction of the  source_data, using MPI when needed. \n\nConsider the binary tree with leaves given by the global indices specified in e.load and stored  in the different MPI processes' input source_data vectors.  At each node of the tree, a reduction is performed using operation, i.e.  by calling operation(left_child, right_child). When, and only when a branch of the tree crosses from one MPI process to another one,  MPI communication is used to transmit the intermediate reduction. \n\nAt the end, for process 1, reduce_deterministically() will return the root of the  binary tree, and for the other processes, reduce_deterministically() will return  nothing. \n\nNote that even when the operation is only approximately associative (typical situation  for floating point reductions), the output of this function is invariant to the  number of MPI processes involved (hence the terminology 'deterministically').  This contrasts to direct use of MPI collective communications where the leaves are  MPI processes and hence will give slightly different outputs given different  numbers of MPI processes. In the context of randomized algorithms, these minor  differences are then amplified. \n\nIn contrast to transmit!(), we do not assume isbitstype(T) == true and use  serialization when messages are transmitted over MPI.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.reduced_recorders-Tuple{Any}","page":"Reference","title":"Pigeons.reduced_recorders","text":"reduced_recorders(replicas) -> Any\n\n\nPerform a reduction across all the replicas' individual recorders,  using  combine() on each individual recorder held.  Returns a recorders with all the information merged. \n\nSince this uses all_reduce_deterministically, the output is  identical, no matter how many MPI processes are used, even when  the reduction involves only approximately associative combine() operations (e.g. most floating point ones).\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.reference_chains-Tuple{Any}","page":"Reference","title":"Pigeons.reference_chains","text":"reference_chains(swap_graph) -> Set{Int64}\n\n\nGiven a swap_graph, return the set of chain(s) targetting the distribution of interest. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.restarts-Tuple{Any}","page":"Reference","title":"Pigeons.restarts","text":"restarts(indices_matrix; cumulative)\n\nCompute the number of restarts for a given index process trajectory.  Otherwise, it is the same as roundtrip().\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.roundtrip-Tuple{Any}","page":"Reference","title":"Pigeons.roundtrip","text":"roundtrip(indices_matrix; cumulative)\n\nCompute the number of round trips for a given index process trajectory.  indices_matrix is a matrix containing information about the index process. cumulative indicates whether we should store the output as a vector containing  information about the number of total round trips up to sample n. If false, the output is a scalar.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.scaled_normal_example-Tuple{Any, Any}","page":"Reference","title":"Pigeons.scaled_normal_example","text":"scaled_normal_example(n_chains, dim) -> Any\n\n\nToy path for testing: see section I.4.1 in Syed et al 2021. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.setkernels-Tuple{Any, Any}","page":"Reference","title":"Pigeons.setkernels","text":"setkernels(potential, etas)\n\nSet the local exploration kernels given the potential and the annealing  parameters, etas.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.single_process_load-Tuple{Any}","page":"Reference","title":"Pigeons.single_process_load","text":"single_process_load(n_global_indices) -> Pigeons.LoadBalance\n\n\nA load balance with only one process.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.slice_accept-Tuple{SS, Any, Vector{Float64}, Any, Float64, Float64, Float64, Int64}","page":"Reference","title":"Pigeons.slice_accept","text":"slice_accept(h::SS, g, x_0, x_1, z, L, R, c)\n\nTest whether to accept the current slice.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.slice_double-Tuple{SS, Any, Vector{Float64}, Float64, Int64}","page":"Reference","title":"Pigeons.slice_double","text":"slice_double(h, g, x_0, z, c)\n\nDouble the current slice.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.slice_sample-Tuple{SS, Vector{Float64}, Int64}","page":"Reference","title":"Pigeons.slice_sample","text":"slice_sample(h::SS, x_0::Vector{Float64}, nsamps::Int)\n\nSlice sample nsamps given a starting vector  x_0 and the struct h  that contains information about the log-density.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.slice_shrink-Tuple{SS, Any, Vector{Float64}, Float64, Float64, Float64, Int64}","page":"Reference","title":"Pigeons.slice_shrink","text":"slice_shrink(h, g, x_0, z, L, R, c)\n\nShrink the current slice.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.split_slice-Tuple{UnitRange, Any}","page":"Reference","title":"Pigeons.split_slice","text":"split_slice(slice::UnitRange, rng) -> Any\n\n\nFrom one splittable random object, one can conceptualize an infinite list of splittable random objects.  Return a slice from this infinite list.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.swap!-Tuple{Any, Any, Any}","page":"Reference","title":"Pigeons.swap!","text":"swap!(pair_swapper, replicas, swap_graph)\n\n\nFor each pair of chains encoded in swap_graph, use  pair_swapper to decide if the pair will swap or not,  and write the changes in-place into replicas (i.e. exchanging  the Replica's chain fields for those that swapped.)\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.swap!-Tuple{Any, EntangledReplicas, Any}","page":"Reference","title":"Pigeons.swap!","text":"swap!(pair_swapper, replicas::EntangledReplicas, swap_graph)\n\n\nEntangled MPI swap! implementation.\n\nThis implementation is designed to support distributed PT with the following guarantees\n\nThe running time is independent of the size of the state space      ('swapping annealing parameters rather than states')\nThe output is identical no matter how many MPI processes are used. In particular,      this means that we can check correctness by comparing to the serial, single-process version.\nScalability to 1000s of processes communicating over MPI (see details below).\nThe same function can be used when a single process is used and MPI is not available.\nFlexibility to extend PT to e.g. networks of targets and general paths.\n\nRunning time analysis:\n\nLet N denote the number of chains, P, the number of processes, and K = textceil(NP),   the maximum number of chains held by one process.  Assuming the running time is dominated by communication latency and  a constant time for the latency of each   peer-to-peer communication, the theoretical running time is O(K).  In practice, latency will grow as a function of P, but empirically, this growth appears to be slow enough that for say P = N = a few 1000s,  swapping will not be the computational bottleneck.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.swap!-Union{Tuple{R}, Tuple{Any, Vector{R}, Any}} where R","page":"Reference","title":"Pigeons.swap!","text":"swap!(pair_swapper, replicas::Array{R, 1}, swap_graph)\n\n\nSingle process, non-allocating swap! implementation. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.swap_decision-Tuple{Any, Int64, Any, Int64, Any}","page":"Reference","title":"Pigeons.swap_decision","text":"swap_decision(\n    pair_swapper,\n    chain1::Int64,\n    stat1,\n    chain2::Int64,\n    stat2\n) -> Bool\n\n\nGiven a pair_swapper, a recorders, the provided chain indices, and  the sufficient statistics computed by swap_stat(), make a swap decision.\n\nBy default, this is done as follows:\n\ncompute the standard swap acceptance probability min(1, exp(stat1.log_ratio + stat2.log_ratio))\nmake sure the two chains share the same uniform by picking the uniform from the chain with the smallest chain index \nswap if the shared uniform is smaller than the swap acceptance probability.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.swap_decision-Tuple{Pigeons.TestSwapper, Int64, Float64, Int64, Float64}","page":"Reference","title":"Pigeons.swap_decision","text":"swap_decision(\n    swapper::Pigeons.TestSwapper,\n    chain1::Int64,\n    stat1::Float64,\n    chain2::Int64,\n    stat2::Float64\n) -> Bool\n\n\nSee TestSwapper.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.swap_stat-Tuple{Any, Replica, Int64}","page":"Reference","title":"Pigeons.swap_stat","text":"swap_stat(\n    pair_swapper,\n    replica::Replica,\n    partner_chain::Int64\n) -> Float64\n\n\nBy default, two sufficient statistics are computed and stored in SwapStat struct:\n\nThe result of calling log_unnormalized_ratio() on pair_swapper\nA uniform number to coordinate the swap decision.\n\nThis can be extended by dispatching on other pair_swapper types, with the  constraint that the returned sufficient statistics should satisfies isbitstype().\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.swap_stat-Tuple{Pigeons.TestSwapper, Replica, Int64}","page":"Reference","title":"Pigeons.swap_stat","text":"swap_stat(\n    swapper::Pigeons.TestSwapper,\n    replica::Replica,\n    partner_chain::Int64\n) -> Float64\n\n\nSee TestSwapper.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.target_chains-Tuple{Any}","page":"Reference","title":"Pigeons.target_chains","text":"target_chains(swap_graph) -> Set{Int64}\n\n\nGiven a swap_graph, return the set of chain(s) targetting the reference distribution. These are typically tractable in the sense that we can sample  i.i.d. from them. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.transmit!-Union{Tuple{T}, Tuple{Entangler, AbstractVector{T}, AbstractVector{Int64}, Vector{T}}} where T","page":"Reference","title":"Pigeons.transmit!","text":"transmit!(\n    e::Entangler,\n    source_data::AbstractArray{T, 1},\n    to_global_indices::AbstractVector{Int64},\n    write_received_data_here::Array{T, 1}\n)\n\n\nUse MPI point-to-point communication to  permute the contents of source_data across MPI processes, writing the permuted data into  write_received_data_here.  The permutation is specified by the load balance in the input argument e as well as the  argument to_global_indices.\n\nMore precisely, assume the Vectors source_data, to_global_indices, and write_received_data_here  are all of the length specified in my_load(e.load). \n\nFor each i, source_data[i] is sent to MPI process p = find_process(e.load, g),  where g = to_global_indices[i] and  written into this p 's write_received_data_here[j], where j = find_local_index(e.load, g)\n\nSee Entangler's comments regarding the requirement that all machines call transmit() the  same number of times and at logically related intervals. \n\nAdditionally, at each micro-iteration, we assume that  {to_global_indices_p : p ranges over the difference processes} forms a partition of  {1, ..., e.load.n_global_indices} If ran in single-process mode, this 'partition property' is checked;  if ran in multi-process, opportunistic checks will be made, namely when several entries in to_global_indices  lie in the same process, but systematic checks are not made for performance reasons. \n\nWe also assume isbitstype(T) == true. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.transmit-Union{Tuple{T}, Tuple{Entangler, AbstractVector{T}, AbstractVector{Int64}}} where T","page":"Reference","title":"Pigeons.transmit","text":"transmit(\n    e::Entangler,\n    source_data::AbstractArray{T, 1},\n    to_global_indices::AbstractVector{Int64}\n) -> Vector\n\n\nThe same as transmit!() but instead of writing the result to an input argument, provide the result  as a returned Vector. \n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.updateschedule-Tuple{Any, Int64}","page":"Reference","title":"Pigeons.updateschedule","text":"updateschedule(\n    cumulativebarrier,\n    N::Int64\n) -> Vector{Float64}\n\n\nUpdate the annealing schedule. Given the cumulative communication barrier function in cumulativebarrier, find the optimal schedule of size N+1.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.winsorized_mean-Tuple{Any}","page":"Reference","title":"Pigeons.winsorized_mean","text":"winsorized_mean(x; α)\n\nCompute the winsorized mean from an input x, which is assumed to be a vector of vectors.  α denotes the percentage of observations to winsorize at the bottom and the top  so that we use 1 - 2α observations and winsorize the rest.\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.winsorized_std-Tuple{Any}","page":"Reference","title":"Pigeons.winsorized_std","text":"winsorized_std(x; α)\n\nCompute the winsorized standard deviation. The parameters are the same  as those for winsorized_mean().\n\n\n\n\n\n","category":"method"},{"location":"reference/#Pigeons.@informal-Tuple{Symbol, Expr}","page":"Reference","title":"Pigeons.@informal","text":"@informal name begin ... end\n\nDocument an informal interface with provided name, and functions  specified in a begin .. end block. \n\n@informal will spit back the contents of the begin .. end block so  this macro can be essentially ignored at first read. \n\nWhen building documentation, this allows us to use the  function informal_doc() to automatically document the  informal interface.\n\n\n\n\n\n","category":"macro"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"CurrentModule = Pigeons","category":"page"},{"location":"distributed/#Distributed-and-parallel-implementation-of-PT","page":"Distributed PT","title":"Distributed and parallel implementation of PT","text":"","category":"section"},{"location":"distributed/#Introduction","page":"Distributed PT","title":"Introduction","text":"","category":"section"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Pigeons provides an implementation of Distributed PT based on Syed et al., 2021,  Algorithm 5. This page describes the challenges of implementing this distributed,  parallelized, and randomized algorithm and how we address these challenges.","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"note: Note\nRead this page if you are interested in extending Pigeons or  understanding how it works under the hood.  Reading this page is not required to use Pigeons. Instead, refer to the  user guide. ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"In Distributed PT, one or several computers run MCMC simulations in parallel and  communicate with each other to improve MCMC efficiency.  We use the terminology machine for one of these computers, or, to be more precise,  process. In the typical setting, each machine will run one process, since our implementation also supports  the use of several Julia threads.","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Pigeons is designed so that it is suitable in all these scenarios:","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"one machine running PT on one thread,\none machine running PT on several threads,\nseveral machines running PT, each using one thread, and\nseveral machines running PT, each using several threads.","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Ensuring code correctness at the intersection of randomized, parallel, and distributed algorithms is a challenge.  To address this challenge, we designed Pigeons based on the following principle:","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"note: Parallelism Invariance\nThe output of Pigeons is invariant to the number of machines and/or threads.","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"In other words, if X_m t(s) denotes the output of Pigeons when provided m machines, t threads  per machine, and random seed s, we guarantee that X_m t(s) = X_m t(s) for all m t. ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Without explicitly keeping Parallelism Invariance in mind during software construction,  parallel/distributed implementations of randomized algorithms will  typically only guarantee EX_m t = EX_m t for all m m t t. While equality in distribution is technically  sufficient, the stronger pointwise equality required by Parallelism Invariance makes  debugging and software validation considerably easier.  This is because the developer can first focus on the fully serial randomized algorithm,  and then use it as an easy-to-compare gold-standard reference for parallel/distributed  implementations.  This strategy is used extensively in Pigeons to ensure correctness.  In contrast, testing equality in distribution, while possible (e.g., see  Geweke, 2004), incurs additional  false negatives due to statistical error. ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Two factors tend to cause violations of Parallelism Invariance: ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Thread-local random number generators (which is unfortunately the default approach to parallel   random number generators in many languages including Julia).\nNon-associativity of floating point operations. As a result, when several workers    perform Distributed reduction of    floating point values, the output of this reduction will be slightly different.    When these reductions are then fed into further random operations, this implies    two randomized algorithms with the same seed but using a different number of workers    will eventually arbitrarily diverge pointwise. ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"One focus in the remainder of this page is to describe how our implementation sidesteps  the two above issues while maintaining the same asymptotic runtime complexity.","category":"page"},{"location":"distributed/#Overview-of-the-algorithm","page":"Distributed PT","title":"Overview of the algorithm","text":"","category":"section"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Let us start with a high-level picture of the basic distributed PT algorithm:","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"using Pigeons\nusing SplittableRandoms\nusing Plots\nimport Base.Threads.@threads\n\nconst n_chains = 20\n\n# initialize sequence of distributions\nconst dim = 8\nconst normal_log_potentials = scaled_normal_example(n_chains, dim)\n\n# initialize replicas\nconst init = Ref(zeros(dim))               # initialize all states to zero\nconst rng = SplittableRandom(1)            \nconst keys = recorder_keys(:index_process) # determines which statistics to keep\n\nfunction simple_distributed_deo(n_iters, log_potentials)\n    replicas = create_entangled_replicas(n_chains, init, rng, true, keys)\n    for iteration in 1:n_iters\n        # communication phase\n        swap!(log_potentials, replicas, deo(n_chains, iteration))\n        # toy local exploration (in this toy e.g. we can do iid for all chains)\n        @threads for replica in locals(replicas)\n            distribution = log_potentials[replica.chain]\n            replica.state = rand(replica.rng, distribution)\n        end\n    end\n    return reduced_recorders(replicas)\nend\n\ndeo_result = simple_distributed_deo(100, normal_log_potentials)\np = index_process_plot(deo_result)\nsavefig(p, \"index_process_dist.svg\"); nothing # hide","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"(Image: )","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Notice the code is almost identical to the single-machine algorithm presented earlier with the only difference being create_vector_replicas is  replaced by create_entangled_replicas. Also, as promised the  output is identical despite a vastly different swap logic.  Indeed, beyond the superficial syntactic similarities between the single process and  distributed code, the behavious of swap! is quite different (this is triggered by multiple dispatch  detecting the different types for  replica in fully serial versus distributed). ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"In the following, we go over the main building block of  our distributed PT algorithm. ","category":"page"},{"location":"distributed/#Splittable-random-streams","page":"Distributed PT","title":"Splittable random streams","text":"","category":"section"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"The first building block is a splittable random stream.  To motivate splittable random streams, consider the following example to illustrate  how thread-local random number generators break Parallelism Invariance:","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"using Pigeons\nusing SplittableRandoms\nusing Random\nimport Base.Threads.@threads\n\nprintln(\"Number of threads: $(Threads.nthreads())\")\n\nconst n_iters = 10000\nresult = zeros(n_iters)\nRandom.seed!(1)\n@threads for i in 1:n_iters\n    # in a real problem, do some expensive calculation here...\n    result[i] = rand()\nend\nprintln(\"Multi-threaded: $(last(result))\")\n\nRandom.seed!(1)\nfor i in 1:n_iters\n    # in a real problem, do some expensive calculation here...\n    result[i] = rand()\nend\nprintln(\"Single-threaded: $(last(result))\")","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Unless only one thread is used, the result of the above parallel loop versus serial loop will be different with  high probability. ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"To work around this, we associate one random number generator to each PT chain instead  of one generator per thread. ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"To do so, we use the  SplittableRandoms.jl library which allows  us to turn one seed into several pseudo-independent random number generators.  Since each MPI process holds a subset of the chains, we internally use the  function split_slice() to  get the random number generators for the slice of replicas held in a given MPI process.","category":"page"},{"location":"distributed/#Distributed-replicas","page":"Distributed PT","title":"Distributed replicas","text":"","category":"section"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Calling create_entangled_replicas will produce a fresh EntangledReplicas,  taking care of distributed random seed splitting internally.  An EntangledReplicas contains the list of replicas that are local to the machine, in addition to three data structures allowing distributed communication:  a LoadBalance which keeps track of  how to split work across machines; an Entangler, which encapsulates MPI calls;  and a PermutedDistributedArray, which   maps chain indices to replica indices. These datastructures can be obtained using load(), entangler(), and  replicas.chain_to_replica_global_indices respectively.","category":"page"},{"location":"distributed/#Distributed-swaps","page":"Distributed PT","title":"Distributed swaps","text":"","category":"section"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"To perform distributed swaps, swap!() proceeds as follows:","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"Use the swap_graph to determine swapping partner chains,\ntranslate partner chains into partner replicas (global indices) using  replicas.chain_to_replica_global_indices,\ncompute swap_stat() for local chains, and use   transmit() to obtain partner swap stats,\nuse swap_decision() to decide if each pair should swap, and \nupdate the replicas.chain_to_replica_global_indices datastructure. ","category":"page"},{"location":"distributed/#Distributed-reduction","page":"Distributed PT","title":"Distributed reduction","text":"","category":"section"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"After each round of PT, the workers need to exchange richer messages compared to the information exchanged in the swaps.  These richer messages include swap acceptance probabilities,  statistics to adapt a variational reference, etc. ","category":"page"},{"location":"distributed/","page":"Distributed PT","title":"Distributed PT","text":"This part of the communication is performed using reduced_recorders() which  in turn calls all_reduce_deterministically() with the appropriate   merging operations. See reduced_recorders() and  all_reduce_deterministically() for more information on how  our implementation preserves Parallelism Invariance, while maintaining the logarithmic runtime of binary-tree based  collective operations (more precisely, all_reduce_deterministically() runs in time log(N)  when each machine holds a single chain).","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"CurrentModule = Pigeons","category":"page"},{"location":"#Pigeons","page":"Guide","title":"Pigeons","text":"","category":"section"},{"location":"","page":"Guide","title":"Guide","text":"Pigeons' core algorithm is a distributed and parallel implementation  of the following algorithms: ","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"Non-Reversible Parallel Tempering (NRPT),    Syed et al., 2021.\nVariational PT, Surjanovic et al., 2022.","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"Pigeons can be used in a multi-threaded context, and/or  distributed over hundreds or thousands of MPI-communicating machines.","category":"page"},{"location":"#Goals","page":"Guide","title":"Goals","text":"","category":"section"},{"location":"","page":"Guide","title":"Guide","text":"We describe here the class of problems that can be approached using Pigeons.","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"Let pi(x) denote a probability density.  In many problems, e.g. in Bayesian statistics, the density pi is typically  known only up to a normalization constant, ","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"pi(x) = fracgamma(x)Z","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"where gamma can be evaluated pointwise, but Z is unknown. Pigeons takes as input the function gamma.","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"terminology: log_potential\nSince we work in log-scale, we use the terminology  log_potential as a shorthand for the  unnormalized log density log gamma(x).  See informal interface log_potential.","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"Pigeons' outputs can be used for two tasks:","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"Approximating expecations of the form Ef(X), where X sim pi.    For example, the choice f(x) = x computes the mean, and    f(x) = Ix in A computes the probability of A under pi.\nApproximating the value of the normalization constant Z. For    example, in Bayesian statistics, this corresponds to the    marginal likelihood.","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"Pigeons shines in the following scenarios:","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"When the posterior density pi is challenging due to    non-convexity and/or concentration on a    sub-manifolds due to unidentifiability.\nWhen the user needs not only Ef(X) but also Z. Many existing MCMC tools   focus on the former and struggle to do the latter in high dimensional    problems. \nWhen the posterior density pi is defined over a non-standard state-space,    e.g. a combinatorial object such as a phylogenetic tree. ","category":"page"},{"location":"#Example","page":"Guide","title":"Example","text":"","category":"section"},{"location":"","page":"Guide","title":"Guide","text":"warning: TODO\nLater on, once we have interfaces with some PPLs, write some user-facing examples,  showing the key capabilities.","category":"page"},{"location":"#Specification-of-general-models","page":"Guide","title":"Specification of general models","text":"","category":"section"},{"location":"","page":"Guide","title":"Guide","text":"The most general way to invoke Pigeons is by specifying two ingredients: a sequence of distributions,  pi_1 pi_2 dots pi_N, and for each pi_i, a pi_i-invariant Markov transition kernel. Typically, pi_1 is a distribution from which we can sample i.i.d. (e.g. the prior, or a variational  approximation), while the last distribution coincides with the distribution of interest,  pi_N = pi.  This sequence of distributions is specified using the informal interface log_potentials. ","category":"page"},{"location":"","page":"Guide","title":"Guide","text":"warning: TODO\nAdd instructions for Markov transition kernels, and example code.","category":"page"},{"location":"#Running-distributed-jobs","page":"Guide","title":"Running distributed jobs","text":"","category":"section"},{"location":"","page":"Guide","title":"Guide","text":"warning: TODO\nAdd instructions once the interface improves a bit.","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"CurrentModule = Pigeons","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"We provide in this page an overview of Non-Reversible Parallel Tempering (PT),  Syed et al., 2021,  linking it with some key parts of the code base. ","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"note: Note\nRead this page if you are interested in extending Pigeons or  understanding how it works under the hood.  Reading this page is not required to use Pigeons, for that instead refer to the  user guide. ","category":"page"},{"location":"pt/#PT-augmented-state-space,-replicas","page":"Parallel Tempering (PT)","title":"PT augmented state space, replicas","text":"","category":"section"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"Let X_n denote a Markov chain on state space mathscrX with stationary distribution pi.  PT is a Markov chain defined on the augmented state space mathscrX^N, hence  a state has the form boldsymbolX = (X^(1) X^(2) dots X^(N)).  Each component of boldsymbolX is stored in a struct called a Replica. ","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"The storage of the vector of replicas boldsymbolX, is done via the informal  interface replicas. In the context of PT running on one computer,  replicas is implemented with a Vector{Replica}. In the context  of running PT distributed across several communicating machines, replicas  is implemented via EntangledReplicas, which stores the parts of  boldsymbolX that are local to that machine as well as data structures  required to communicate with the other machines. ","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"Internally, PT operates on a discrete set of distributions,  pi_1 pi_2 dots pi_N, where N can be obtained using n_chains_global().  We use the terminology chain to refer to an index i of pi_i. Typically, pi_N coincides with the distribution of interest pi (called the \"target\"), while  pi_1 is a tractable approximation that will help PT efficiently explore the  state space (called the \"reference\").  More broadly, we assume a subset of the chains (given by target_chains()) coincide with the target, and that a subset of the chains (given by reference_chains()) support  efficient exploration such as i.i.d. sampling or a rapid mixing kernel. ","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"PT is designed so that its stationary distribution is boldsymbolpi = pi_1 times pi_2 times dots pi_N.  As a result, subsetting each sample to its component corresponding to pi_N = pi,  and applying an integrable function f to each, will lead under weak assumptions  to Monte Carlo averages that converge to the expectation of interest Ef(X) for  X sim pi.","category":"page"},{"location":"pt/#Local-exploration-and-communication","page":"Parallel Tempering (PT)","title":"Local exploration and communication","text":"","category":"section"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"PT alternates between two phases, each boldsymbolpi-invariant: the local  exploration phase and the communication phase. Informally, the first phase attempts to achieve  mixing for the univariate statistics pi_i(X^(i)), while the second phase attempts to  translate well-mixing of these univariate statistics into global mixing of X^(i) by  leveraging the reference distribution(s).","category":"page"},{"location":"pt/#Local-exploration","page":"Parallel Tempering (PT)","title":"Local exploration","text":"","category":"section"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"In the local exploration phase, each Replica's state is modified using a pi_i-invariant kernel,  where i is given by Replica.chain. Often, Replica.chain corresponds to  an annealing parameter beta_i but this need not be the case (see  e.g. Baragatti et al., 2011). The kernel can either modify Replica.state in-place, or modify the  Replica's state field.","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"warning: TODO\nMore details about local exploration once the architecture of that  part of the code is more fleshed out...","category":"page"},{"location":"pt/#Communication","page":"Parallel Tempering (PT)","title":"Communication","text":"","category":"section"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"In the communication phase, PT proposes swaps between pairs of replicas.  These swaps allow each replica's state to periodically visit reference chains. During these reference visits, the state can move around the space quickly.  In principle, there are two equivalent ways to do a swap: the Replicas could exchange  their state fields; or alternatively, they could exchange their chain fields. Since we provide distributed implementations, we use the latter as it ensures that  the amount of data that needs to be exchanged between two machines during a swap  can be made very small (two floats).  It is remarkable that this cost does not vary with the dimensionality of the state space,  in constrast to the naive implementation which would transmit states over the network. See Distributed PT for more information on our distributed implementation.","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"Both in distributed and single process mode,  swaps are performed using the function swap!(). See the documentation there for more information.","category":"page"},{"location":"pt/#Basic-PT-algorithm","page":"Parallel Tempering (PT)","title":"Basic PT algorithm","text":"","category":"section"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"Here is a simplified example of how Algorithm 1 in Syed et al., 2021  can be implemented in Pigeons. (This example is for pedagogy and/or those interested in extending  the library. Users of the library should instead follow higher-level instructions in the user guide page.)","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"using Pigeons\nusing SplittableRandoms\nusing Plots\nimport Base.Threads.@threads\n\nconst n_chains = 20\n\n# initialize sequence of distributions\nconst dim = 8\nconst normal_log_potentials = scaled_normal_example(n_chains, dim)\n\n# initialize replicas\nconst init = Ref(zeros(dim))               # initialize all states to zero\nconst rng = SplittableRandom(1)            # specialized rng (see Distributed PT page)\nconst keys = recorder_keys(:index_process) # determines which statistics to keep\n\nfunction simple_deo(n_iters, log_potentials)\n    replicas = create_vector_replicas(n_chains, init, rng, keys)\n    for iteration in 1:n_iters\n        # communication phase\n        swap!(log_potentials, replicas, deo(n_chains, iteration))\n        # toy local exploration (in this toy e.g. we can do iid for all chains)\n        @threads for replica in locals(replicas)\n            distribution = log_potentials[replica.chain]\n            replica.state = rand(replica.rng, distribution)\n        end\n    end\n    return reduced_recorders(replicas)\nend\n\ndeo_result = simple_deo(100, normal_log_potentials)\np = index_process_plot(deo_result)\nsavefig(p, \"index_process.svg\"); nothing # hide","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"(Image: )","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"The code above illustrates the two steps needed to collect statistics from the execution of a PT algorithm: ","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"We specify which statistics to collect using recorder_keys() (by    default, those that can be computed in constant memory only are included,    those that have growing memory consumption, e.g. tracking the full    index process as done here, need to be explicitly specified in advance).\nUsing reduced_recorders() to compile the statistics collected    by the different replicas.","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"An object responsible for accumulating all different types of statistics for  one replica is called a  recorders. An object accumulating one  type of statistic for one replica is a recorder.  Each replica has a single recorders to ensure thread safety (as illustrated above  by the use of a parallel local exploration phase using @thread) and to enable distributed  computing. ","category":"page"},{"location":"pt/#Adaptation-and-schedule-update","page":"Parallel Tempering (PT)","title":"Adaptation and schedule update","text":"","category":"section"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"PT requires as input a discrete set of probability distribution, i.e. log_potentials.  How can those be automatically computed from just knowing the reference and target  distributions? This section outlines this process.","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"The starting point is a path object, which is a continuum of distributions.  A path is typically obtained via create_path().  We can also get a toy example consisting of normal distributions with varying  precision parameters via scaled_normal_example(), which is what we  will use here.","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"We now move to a simplified version of Algorithms 2 and 3 in Syed et al., 2021  (again for pedagogy and/or those interested in extending the library), which are  algorithms for adaptively discretizing a continuum of distributions.","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"The algorithm starts with a simple initial discretization. Here it is one where each grid is equally spaced, being built using Schedule() and discretize():","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"# continues from the above\npath = ScaledPrecisionNormalPath(dim)\nschedule = Schedule(n_chains)\nlog_potentials = discretize(path, schedule)\nnothing # hide","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"we then run one round of Algorithm 1, and use its output to  compute an initial estimate of the communication barriers as defined  in Section 4 of Syed et al., 2021  and implemented in communicationbarrier().","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"# continues from the above\ndeo_result = simple_deo(100, log_potentials)\nbarriers = communicationbarrier(deo_result, schedule)\nplot(barriers.cumulativebarrier, legend = false)\nxlims!(0, 1)\nsavefig(\"barrier.svg\") # hide\nbarriers.globalbarrier","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"(Image: )","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"We can then create a new schedule from the cumulative communication barrier  by following Algorithm 2 of Syed et al., 2021  and implemented in Schedule().  Finally, following Algorithm 4 of Syed et al., 2021  we can iterate this process by performing several rounds of PT, each with increasing budget:","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"# continues from the above\n\nfunction adapt(schedule, n_iters)\n    log_potentials = discretize(path, schedule)\n    deo_result = simple_deo(n_iters, log_potentials)\n    barriers = communicationbarrier(deo_result, schedule)\n    plot!(barriers.cumulativebarrier)\n    xlims!(0, 1)\n    return (Schedule(n_chains, barriers.cumulativebarrier), barriers)\nend\n\nfunction nrpt(schedule)\n    n_iters = 2\n    for round_index in 1:10\n        schedule, barriers = adapt(schedule, n_iters)\n        n_iters *= 2\n    end\n    return barriers\nend\n\nplot()\nbarriers = nrpt(schedule)\n\nsavefig(\"barriers.svg\"); nothing # hide","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"(Image: )","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"The simple normal model we are using has a known closed-form expression  for the cumulative barrier. We can use this closed-form expression to check the  accuracy of our PT-derived approximation:","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"# continues from the above\nanalytic = analytic_cumulativebarrier(path)\nplot([analytic, barriers.cumulativebarrier], labels = [\"analytic\" \"estimate\"])\nxlims!(0, 1)\nsavefig(\"compare-barriers.svg\"); nothing # hide","category":"page"},{"location":"pt/","page":"Parallel Tempering (PT)","title":"Parallel Tempering (PT)","text":"(Image: )","category":"page"}]
}
